[{"content":"本文将介绍Bernardi–Raugel有限元到$\\mathbb{RT}_{0,2}$和\\(\\mathbb{BDM}_{1,2}\\)的插值算子，为代码实现做准备。\nBernardi–Raugel有限元（二维） 多项式空间： \\[\\mathbb{BR}:=\\mathbf{P}_{1,2}\\oplus\\mathrm{Span}\\{\\mathbf{p}_i:\\mathbf{p}_i= \\mathbf{n}_i\\prod_{j=1,j\\ne i}^{3}\\lambda_j,\\quad i=1,2, 3\\}\\] 其中\\(\\{\\lambda_i\\}_{i=1}^{3}\\)为重心坐标。\n自由度：\n顶点：$\\sigma_{V,i,j}=\\mathbf{v}(a_i)\\cdot\\cuti{e}_j,\\quad i=1,2,3,\\ j=1,2$. 边：$\\sigma_{F,i}=\\int_{F_i}\\mathbf{v}\\cdot\\mathbf{n}_{F_i}\\d s,\\quad i=1,2,3$. 整体的有限元空间\\(\\mathbb{BR}_h:=\\{\\mathbf{v}\\in \\mathbf{C}^0(\\overline\\Omega):\\mathbf{v}|_K\\in\\mathbb{BR}(K),K\\in\\mathcal{K}_h\\}\\)是连续的分片多项式，因此$\\mathbb{BR}_h\\subset \\mathbb{H}_0^1$。\n投影算子 设\\(\\Pi^{RT}:\\mathbb{BR}_h\\rightarrow \\mathbb{RT}_{0,2}\\)、\\(\\Pi^{BDM}:\\mathbb{BR}_h\\rightarrow \\mathbb{BDM}_{1,2}\\)为投影算子，\n$\\int_F (\\mathbf{v}_h-\\Pi^{RT}\\mathbf{v}_h)\\cdot\\mathbf{n}_F\\d s=0$.\n这里用到\\(\\mathbb{BR}_h\\)在单元边界两侧的积分$\\int_{F^{\\pm}}\\mathbf{v}\\cdot\\mathbf{n}_F\\d s$相同（\\(\\mathbb{BR}_h\\)是连续元），否则不能定义$\\int_F$。\n\\[\\int_F(\\mathbf{v}_h-\\Pi^{BDM}\\mathbf{v}_h)\\cdot \\mathbf{n}_Fq_h\\d s=0,\\quad F\\in\\mathcal{F}_h,q_h\\in\\mathbf{P}_{1,1}(F).\\] 这里的良定性同样由$\\mathbb{BR}_h$是连续元保证。\n$\\mathrm{div}_h=\\pi_0\\circ \\mathrm{div}:\\mathbf{H}_0^1\\rightarrow \\mathbb{Q}_h$，其中$\\mathbb{Q}_h$为分片常数空间，$\\pi_0$为$L^2$到$\\mathbb{Q}_h$的$L^2$投影，满足$(r,q_h)=(\\pi_0(r),q_h),\\quad \\forall r\\in L^2, q_h\\in \\mathbb{Q}_h$。\nProp. $\\forall \\mathbf{v}_h\\in\\mathbb{BR}_h$， $\\mathrm{div}_h\\mathbf{v}_h=\\mathrm{div}(\\Pi \\mathbf{v}_h)$，其中$\\Pi=\\Pi^{RT},\\Pi^{BDM}$。\nProof：\n显然$\\mathrm{div}_h\\mathbf{v}_h=\\mathrm{div}(\\Pi \\mathbf{v}_h)$两者都是分片常数，$\\forall K\\in\\mathcal{K}_h$， \\[\\begin{aligned}\u0026\\int_K\\mathrm{div}_h\\mathbf{v}_h\\d\\mathbf{x}\\\\=\u0026\\int_K\\mathrm{div}\\mathbf{v}_h\\d \\mathbf{x}\\\\ =\u0026\\int_{\\p K}\\mathbf{v}_h\\cdot \\mathbf{n}\\d s\\\\ =\u0026\\int_{\\p K}\\Pi\\mathbf{v}_h\\cdot\\mathbf{n}\\d s\\\\ =\u0026\\int_{K}\\mathrm{div}(\\Pi \\mathbf{v}_h)\\d \\mathbf{x} \\end{aligned}\\] 因此， \\[(\\mathrm{div}_h\\mathbf{v}_h)_K=(\\mathrm{div}(\\Pi \\mathbf{v}_h))_K,\\quad \\forall K\\in\\mathcal{K}_h.\\]代码实现 \\(\\mathbb{BR}_h\\)沿单元边界连续，因此逐单元投影到\\(\\mathbb{RT}_{0,2}(K)\\)和\\(\\mathbb{BDM}_{1,2}(K)\\)得到的分片多项式分别在\\(\\mathbb{RT}_{0,2}\\)和\\(\\mathbb{BDM}_{1,2}\\)中。\n$\\mathbb{BR}(K)$的一组基为 \\[\\begin{cases}\\cuti{\\varphi}_{i}=\\lambda_i\\mathbf{e}_{1},\\quad i=1,2,3,\\\\ \\cuti{\\varphi}_{i+3}=\\lambda_i\\mathbf{e}_2,\\quad i=1,2,3,\\\\ \\cuti{\\varphi}_{i+6}=\\mathbf{n}_i\\prod_{j=1,j\\ne i}^{3}\\lambda_j,\\quad i=1,2,3. \\end{cases} \\]$\\Pi^{RT}$ $\\lambda_i$在边$F_j$均值（中点值）为\\(\\frac12(1-\\delta_{ij})\\).\n$\\Pi^{RT}$插值可显式表示为： \\[\\Pi^{RT}\\cuti{v}:=\\cuti{a}_K+\\frac{c_K}{2}(\\cuti{x}-\\cuti{x}_K)\\tag{1}\\] 其中 \\[\\begin{aligned}\u0026c_K=\\frac{1}{|K|}\\int_{\\p K}\\cuti{v}\\cdot\\cuti{n}\\d s,\\\\\u0026\\cuti{a}_K=\\frac{1}{|K|}\\sum_{F\\subset \\p K}\\int_F\\cuti{v}\\cdot\\cuti{n}\\d s(\\cuti{x}_F-\\cuti{x}_K),\\end{aligned}\\] 其中$\\cuti{x}_K$和$\\cuti{x}_F$分别是单元和边的重心。\n利用公式(1)， \\[\\begin{aligned}\u0026\\Pi^{RT}\\cuti{\\varphi}_{i+3(j-1)}\\\\ =\u0026\\frac{1}{|K|}\\left(\\sum_{k\\ne i}\\frac12|F_k|(\\mathbf{x}_{F_k}-\\mathbf{x}_K)+\\frac{\\sum_{k\\ne i}\\frac12|F_k|}{2}(\\cuti{x}-\\cuti{x}_K)\\right),\\quad i=1,2,3,\\ j=1,2,\\\\ \u0026\\Pi^{RT}\\cuti{\\varphi}_{i+6}\\\\ =\u0026\\frac{|F_i|}{6|K|}\\left((\\cuti{x}_{F_i}-\\cuti{x}_K)+\\frac12(\\cuti{x}-\\cuti{x}_K)\\right),\\quad i=1,2,3.\\end{aligned}\\]$\\Pi^{BDM}$ 由 \\[\\varphi_i\\in \\mathbb{BDM}_{1,2}\\quad i=\\{1,2,\\cdots,6\\}\\] 可知 \\[\\Pi^{BDM}\\mathbf{\\varphi}_i=\\varphi_i,\\quad i=\\{1,2,\\cdots,6\\}.\\] 下面考虑$\\Pi^{BDM}\\varphi_i,\\ i=7,8,9$。\n在参考单元上定义自由度如下： \\[\\begin{aligned}\\hat\\sigma_1(\\cuti{\\hat v})=\\int_{\\hat{F}_1}{\\cuti{\\hat v}}\\cdot\\cuti{\\hat n}_{\\hat{F}_1}\\hat\\lambda_2\\d \\hat s,\\hat\\sigma_2(\\cuti{\\hat v})=\\int_{\\hat F_1}\\cuti{\\hat v}\\cdot\\cuti{\\hat n}_{\\hat F_1}\\hat \\lambda_3\\d \\hat s\\\\ \\hat\\sigma_3(\\cuti{\\hat v})=\\int_{\\hat F_2}\\cuti{\\hat v}\\cdot\\cuti{\\hat n }_{\\hat F_2}\\hat \\lambda_1\\d \\hat s,\\hat\\sigma_4(\\cuti{\\hat v})=\\int_{\\hat F_2}\\cuti{v}\\cdot\\cuti{\\hat n}_{\\hat F_2}\\hat \\lambda_3\\d \\hat s\\\\ \\hat\\sigma_5(\\cuti{\\hat v})=\\int_{\\hat F_3}\\cuti{\\hat v}\\cdot\\cuti{\\hat n}_{\\hat F_3}\\hat \\lambda_1\\d \\hat s,\\hat\\sigma_6(\\cuti{\\hat v})=\\int_{\\hat F_3}\\cuti{\\hat v}\\cdot\\cuti{\\hat n}_{\\hat F_3}\\hat \\lambda_2\\d \\hat s\\\\ \\end{aligned}\\]fealpy中计算基函数梯度的方法是先计算各单元基函数关于$(\\lambda_2,\\lambda_3)$的梯度，然后利用 \\[\\cuti{x}=\\sum_{i=1}^3\\cuti{x}_i\\lambda_i=\\cuti{x}_1+(\\cuti{x}_2-\\cuti{x}_1)\\lambda_2+(\\cuti{x}_3-\\cuti{x}_1)\\lambda_3\\] 获得关于实际坐标$\\cuti{x}$的梯度。\n在坐标系$\\lambda_2-\\lambda_3$下，参考单元是以$(0,0),(1,0),(0,1)$作为顶点的等腰直角三角形。\n由Piola变换， \\[\\int_F (\\mathbf{v}\\cdot\\mathbf{n}|_F)(\\mathbf{x})q(\\mathbf{x})\\d s=\\int_{\\hat F} (\\boldsymbol{\\psi}^d_K(\\mathbf{v})\\cdot\\mathbf{\\hat{n}}|_{\\hat F})(\\mathbf{\\hat{x}})\\psi_K^g(q)(\\mathbf{\\hat{x}})\\d \\hat s\\] 其中 \\[\\begin{aligned} \u0026\\psi_K^g(q)=v\\circ \\cuti{T}_K(\\hat{\\cuti{x}})\\\\ \u0026\\cuti{\\psi}_K^d(\\cuti{v})=\\mathrm{det}(\\mathbb{J}_K)\\mathbb{J}_K^{-1}\\mathbf{v}\\circ\\mathbf{T}_K(\\hat{\\cuti{x}}) \\end{aligned}\\] 考虑单元$K$上自由度（变量替换到参考元上可证线性独立性）： \\[\\begin{aligned}\\sigma_1(\\cuti{v})=\\int_{F_1}\\left(\\frac{1}{\\mathrm{det}(\\mathbb{J}_K)}\\mathbb{J}_K\\cuti{v}\\right)\\cdot\\cuti{n}_{F_1}\\lambda_2\\d s,\\sigma_2(\\cuti{v})=\\int_{F_1}\\left(\\frac{1}{\\mathrm{det}(\\mathbb{J}_K)}\\mathbb{J}_K\\cuti{v}\\right)\\cdot\\cuti{n}_{F_1}\\lambda_3\\d s\\\\ \\sigma_3(\\cuti{v})=\\int_{F_2}\\left(\\frac{1}{\\mathrm{det}(\\mathbb{J}_K)}\\mathbb{J}_K\\cuti{v}\\right)\\cdot\\cuti{n}_{F_2}\\lambda_1\\d s,\\sigma_4(\\cuti{v})=\\int_{F_2}\\left(\\frac{1}{\\mathrm{det}(\\mathbb{J}_K)}\\mathbb{J}_K\\cuti{v}\\right)\\cdot\\cuti{n}_{F_2}\\lambda_3\\d s\\\\ \\sigma_5(\\cuti{v})=\\int_{F_3}\\left(\\frac{1}{\\mathrm{det}(\\mathbb{J}_K)}\\mathbb{J}_K\\cuti{v}\\right)\\cdot\\cuti{n}_{F_3}\\lambda_1\\d s,\\sigma_6(\\cuti{v})=\\int_{F_3}\\left(\\frac{1}{\\mathrm{det}(\\mathbb{J}_K)}\\mathbb{J}_K\\cuti{v}\\right)\\cdot\\cuti{n}_{F_3}\\lambda_2\\d s\\\\ \\end{aligned}\\] 则 \\[\\sigma_i(\\cuti{v})=\\hat{\\sigma}_i(\\hat{\\cuti{v}}),\\quad i=1,2,\\cdots,6\\]显然\\(\\{\\cuti{\\varphi}_i\\}_{i=1}^6\\)是$\\mathbb{BDM}_{1,2}$的一组基，设\\(\\Pi^{BDM}\\cuti{v}=\\sum_{i=1}^6b_i\\cuti{\\varphi}_i\\)。 令 \\[A:=(\\sigma_i(\\cuti{\\varphi}_j))=(\\hat{\\sigma}_i(\\cuti{\\hat{\\varphi}}_j)),\\] 则 \\[A\\vec{b}=\\vec{f}^{\\cuti{v}},\\quad {f}^{\\cuti{v}}_i=\\sigma_i(\\cuti{v}).\\] 其中$A$为固定矩阵。 由于 \\[\\int_{F_k}\\lambda_i^p\\lambda_j^q\\d s=\\frac{p!q!}{(p+q+1)!}|F_k|,\\quad (i,j,k\\text{互不相同})\\tag{2}\\] 通过在参考元上计算（利用公式(2)）可得： \\[\\begin{aligned} A\u0026=\\begin{pmatrix} 0\u0026-\\frac13\u0026-\\f16\u00260\u00260\u00260\\\\ 0\u0026-\\frac16\u0026-\\f13\u00260\u00260\u00260\\\\ 0\u00260\u00260\u0026-\\frac13\u00260\u0026-\\f16\\\\ 0\u00260\u00260\u0026-\\frac16\u00260\u0026-\\f13\\\\ \\f{1}{3}\u0026\\f1{6}\u00260\u0026\\f{1}{3}\u0026\\f1{6}\u00260\\\\ \\f{1}{6}\u0026\\f1{3}\u00260\u0026\\f{1}{6}\u0026\\f1{3}\u00260\\\\ \\end{pmatrix},\\\\ A^{-1}\u0026=\\begin{pmatrix} -2\u00264\u00260\u00260\u00264\u0026-2\\\\ -4\u00262\u00260\u00260\u00260\u00260\\\\ 2\u0026-4\u00260\u00260\u00260\u00260\\\\ 0\u00260\u0026-4\u00262\u00260\u00260\\\\ 4\u0026-2\u00260\u00260\u0026-2\u00264\\\\ 0\u00260\u00262\u00264\u00260\u00260\\\\ \\end{pmatrix}.\\end{aligned}\\]再次利用公式(1)计算可得： \\[\\begin{aligned}\\vec{f}^{\\cuti{\\varphi}_7}=\\begin{pmatrix}\\frac1{12}\\\\\\f{1}{12}\\\\0\\\\0\\\\0\\\\0\\end{pmatrix},\\ \\vec{f}^{\\cuti{\\varphi}_8}=\\begin{pmatrix}0\\\\0\\\\\\frac1{12}\\\\\\f{1}{12}\\\\0\\\\0\\end{pmatrix},\\ \\vec{f}^{\\cuti{\\varphi}_9}=\\begin{pmatrix}0\\\\0\\\\0\\\\0\\\\\\frac1{12}\\\\\\f{1}{12}\\end{pmatrix}.\\end{aligned}\\]于是 \\[\\Pi^{BDM}\\cuti{\\varphi}_{i}=(\\cuti{\\varphi}_1,\\cdots,\\cuti{\\varphi}_6)A^{-1}\\vec{f}^{\\cuti{\\varphi}_i},\\quad i=7,8,9.\\]Remarks 我们可以在初始化阶段将基函数的插值计算并存储，即存储phi,Pi_RT_phi,Pi_BDM_phi等变量。\n插值算子 本节考虑将光滑函数插值到$\\mathbb{BR}$空间，该空间全局自由度为： \\[\\begin{aligned}\u0026\\sigma^G_{V,i,j}(\\cuti{v})=\\cuti{v}(\\cuti{x}_i)\\cdot\\cuti{e}_j,\\quad\\cuti{x}_i\\in\\mathcal{V}_h,\\ j=1,2,\\\\ \u0026\\sigma^G_{F,i}(\\cuti{v}) = \\frac{1}{|F_i|}\\int_{F_i}\\cuti{v}\\cdot\\cuti{n}\\d s,\\quad F_i\\in \\mathcal{E}_h. \\end{aligned}\\] 其中$\\mathcal{V}_h,\\mathcal{E}_h$分别是网格的顶点集和边集，$G$表示global。\n当$\\cuti{v}$为连续函数时，在任意$F$上$\\cuti{v}$连续，我们同样可以逐单元插值。在$\\mathbb{BR}(K)$上，取 \\[\\begin{aligned} \u0026\\cuti{\\psi}_i = \\cuti{\\varphi}_i-\\frac{1}2\\sum_{j\\ne i}\\left((\\cuti{n}_{F_j}\\cdot\\cuti{e}_1)(6\\cuti{\\varphi}_{j+6})\\right),\\quad i=1,2,3,\\\\ \u0026\\cuti{\\psi}_{i+3} = \\cuti{\\varphi}_{i+3}-\\frac{1}2\\sum_{j\\ne i}\\left((\\cuti{n}_{F_j}\\cdot\\cuti{e}_2)(6\\cuti{\\varphi}_{j+6})\\right),\\quad i=1,2,3,\\\\ \u0026\\cuti{\\psi}_{i+6} = 6\\cuti{\\varphi}_{i+6},\\quad i=1,2,3, \\end{aligned}\\] 作为一组基。取单元自由度如下： \\[\\begin{aligned}\u0026\\sigma_i=\\sigma_{V,i,1},\\quad i=1,2,3,\\\\ \u0026\\sigma_{i+3}=\\sigma_{V,i,2},\\quad i=1,2,3,\\\\ \u0026\\sigma_{i+6}=\\frac{1}{|F_i|}\\sigma_{F,i},\\quad i=1,2,3,\\\\ \\end{aligned}\\] 则 \\[\\sigma_i(\\cuti{\\psi}_j)=\\delta_{ij},\\quad i,j=1,2,\\cdots,9.\\]参考文献 Piola变换. ","date":"2025-08-07T20:53:42+08:00","permalink":"https://baichuan-blog.netlify.app/p/projection-from-br-to-rt-and-bdm/","title":"Projection from BR to RT and BDM"},{"content":"不动点定理至少有以下三个比较有用的类别：\nfor strict contractions——Banach\u0026rsquo;s fixed point theorem for compact mappings——Schauder\u0026rsquo;s and Schaefer\u0026rsquo;s fixed point theorems for order-preserving operators Brouwer不动点定理 分析证法[1, Sect. 8.1] Null Lagrangians \\[L=L(P,z,x)=L(p_1^1,\\cdots, p_n^m,z^1,\\cdots,z^m,x_1,\\cdots,x_n)\\] 其中$P\\in\\mathbb{M}^{m\\times n},z\\in\\R^m, x\\in U$.\nNull Lagrangian的重要性在于相应的能量泛函 \\[I[\\mathbf{w}]=\\int_UL(D\\mathbf{w},\\mathbf{w},x)\\d x\\] 仅依赖于边值条件。\n当$m=1$时，只有一类平凡的null Lagrangian，即$L$是关于$p$是线性的线性函数。\n\\[L(P)=\\mathrm{det}(P),\\quad(P\\in\\mathbb{M}^{n\\times n})\\] 是一个null Lagrangian，利用 \\[\\sum_{i=1}^n (\\mathrm{cof}\\ Du)_{k,i,x_i}=0,\\quad (k=1,\\cdots,n)\\] 证明。\nBrouwer不动点定理\n假设$\\mathbf{u}:B(0,1)\\rightarrow B(0,1)$是连续的，其中$B(0,1)$是$\\R^n$中的单位闭球，则$\\mathbf{u}$在$B(0,1)$中存在不动点。\nProof：\n不存在光滑函数\\(\\mathbf{w}: B\\rightarrow\\p B\\)使得$\\mathbf{w}(\\mathbf{x})=\\mathbf{x},\\ \\forall\\mathbf{x}\\in\\p B$. 若存在这样的$\\mathbf{w}$，则其与恒等映射$\\mathbf{id}$在边界处相等，取null Lagrangian $L(P)=\\mathbf{det}(P)$，此时 \\[I[\\mathbf{w}]=I[\\mathbf{id}]=\\int_{U}1\\d x=|U|\\ne 0.\\] 另一方面，由于$\\mathbf{w}^T\\mathbf{w}=1$，所以$(D\\mathbf{w})^T\\mathbf{w}=0$。对于任意$x\\in U$，由于$\\norm{\\mathbf{w}}=1$，因此线性方程组的系数矩阵$(D\\mathbf{w})^T$行列式为0，矛盾！ 不存在连续函数\\(\\mathbf{w}: B\\rightarrow\\p B\\)使得$\\mathbf{w}(\\mathbf{x})=\\mathbf{x},\\ \\forall\\mathbf{x}\\in\\p B$。 若连续函数\\(\\mathbf{u}: B\\rightarrow B\\)不存在不动点，则$u(\\mathbf{x})\\ne\\mathbf{x},\\forall \\mathbf{x}$，定义$\\mathbf{w}$为$\\mathbf{x}$和$\\mathbf{u}(\\mathbf{x})$所在直线与$\\p B$的交点，则$\\mathbf{w}(x)$连续，且$\\mathbf{w}(\\mathbf{x})=\\mathbf{x},\\ \\forall\\mathbf{x}\\in\\p B$。与2矛盾，因此$\\mathbf{u}(\\mathbf{x})$存在不动点。 组合证法[2] Sprener染色 对于任意$n$维单纯形，考虑其上的单纯形剖分，我们采用如下方式对剖分得到的所有顶点进行染色：\n$n+1$个顶点用$n+1$种不同的颜色； 对于任意子单纯形${i_1,i_2,\\cdots,i_k}$，其中$k=1,2,\\cdots,n$，$i_1,i_2,\\cdots,i_k\\in{1,2,\\cdots,n+1}$，若一个顶点落在该子单纯形中，则任选顶点集${i_1,i_2,\\cdots,i_k}$所染颜色中的一种对该点进行染色。 Sprener引理 $n$维单纯形中的任意一个Sprener染色，必定存在一个子单纯形的顶点集包含所有的$n+1$种颜色。\nProof：\n当$n=1$时，单纯形是一个区间，不妨记为$[0,1]$，区间两端点分别为不同颜色，从左到右顶点的变色次数必为奇数，且端点颜色不同子区间有奇数个。因此必定存在一个子区间，其两个端点颜色不同。 假设对于$n=k$，彩虹单纯形（顶点颜色互不相同）数目为奇数。当$n= k+1$时，我们利用归纳法进行证明。设$R$是彩虹单纯形个数，$Q$是恰好有${1,2,\\cdots,n}$这$n$种颜色的单纯形个数，$X$是原始单纯形的表面上包含颜色${1,2,\\cdots,n}$的面片个数，$Y$是内部面中恰好包含颜色${1,2,\\cdots,n}$的面片个数，则观察恰好包含颜色${1,2,\\cdots,n}$的面片数量可以得到下面关系式： $$R+2Q= X+2Y.$$ 由于原始单纯形表面上包含颜色${1,2,\\cdots,n}$的面片只出现在顶点${1,2,\\cdots,n}$上，而由归纳假设，该$n-1$维单纯形上的$n-1$维彩虹单纯形有奇数个，即$X$为奇数，因此$R$也为奇数。 综上，我们证得彩虹单纯形个数为奇数，因此必定存在。 Brouwer不动点定理 设$M\\subset \\mathbb{R}^n$是有界凸闭集，$F:M\\rightarrow M$是连续函数，则存在不动点$\\mathbf{x}_0\\in M$，使得$F(\\mathbf{x}_0)=\\mathbf{x}_0$。\nProof：\n对于一般的$M$，我们可取包含$M$的单纯形$B$，将$F$延拓为$B$上函数$\\tilde{F}:B\\rightarrow B$， $$\\tilde{F}(\\mathbf{x}) = \\argmin_{\\mathbf{y}\\in M}\\norm{\\mathbf{x}-\\mathbf{y}}.$$ 由于$M$是凸闭集，因此$\\tilde{F}$是良定的（右端存在唯一的$y\\in M$使得$\\norm{\\mathbf{x}-\\mathbf{y}}$极小），且连续。 若$\\mathbf{x}\\in M$，则$\\tilde{F}(\\mathbf{x}) = \\mathbf{x}$，且$\\tilde{F}(B)\\subset M\\subset B$。于是，只要对单纯形的情形证明了结论，则存在$\\mathbf{x}_0\\in B$，使得$\\mathbf{x}_0=\\tilde{F}(\\mathbf{x}_0)\\in M$。\n不妨设$M$为$n$维单纯形，记\\(\\{\\mathbf{v}_i\\}_{i=1}^{n+1}\\)为$M$的$n+1$个顶点。设\\(\\{\\lambda_i(\\mathbf{x})\\}_{i=1}^{n+1}\\)是点$\\mathbf{x}\\in M$的重心坐标，则$\\sum_{i=1}^{n+1}\\lambda_i= 1$。定义\\(\\tilde{\\lambda}_i(\\mathbf{x}) = \\lambda_i({F}(\\mathbf{x}))\\)，则\\(\\sum_{i=1}^{n+1}\\tilde{\\lambda}_i=1\\)。考虑对$M$进行嵌套的单纯形剖分，使得单纯形直径趋于0。由于\\(\\sum_{i}\\lambda_i=\\sum_i\\tilde{\\lambda}_i=1\\)，因此剖分中的单纯形顶点可考虑按照顶点$\\mathbf{v}_{\\argmin{i:\\lambda_i\\ge \\tilde{\\lambda_i}}}$染色（对于$M$的顶点$\\mathbf{v}_i$，只有$\\lambda_i=1$，因此恰好是根据其自身染色，这是自洽的）。这是Sprener染色。\n由Sprener引理，存在彩虹单纯形序列$A_1\\supset A_2\\supset A_{k-1}\\supset A_k\\supset \\cdots$，$A_i$的直径趋于0。对于$A_i$，各顶点适当排序后记为\\(\\{\\mathbf{v}_{j}^i\\}_{j=1}^{n+1}\\)，有 \\[\\lambda_j(\\mathbf{v}_j^i)\\ge\\tilde\\lambda_j(\\mathbf{v}_j^i)\\] 由致密性定理，对每个$j$，存在$\\mathbf{v}_j^{\\ast}$，使得\\(\\tilde\\lambda(\\mathbf{v}_j^{\\ast})\\le \\lambda(\\mathbf{v}_j^{\\ast}) \\)。又由闭区间套定理可知所有\\(\\mathbf{v}_j^{\\ast}\\)都恰好是彩虹单纯形序列的极限点$\\mathbf{v}^*$。因此\n\\[\\tilde\\lambda_j(\\mathbf{v}^{\\ast})\\le \\lambda_j(\\mathbf{v}^{\\ast}),\\quad j=1,2,\\cdots, n+1.\\] 由于 \\[\\sum_{j=1}^{n+1}\\tilde\\lambda_j=\\sum_{j=1}^{n+1}\\lambda_j=1,\\] 因此 \\[\\tilde\\lambda_j(\\mathbf{v}^{\\ast})= \\lambda_j(\\mathbf{v}^{\\ast}),\\quad j=1,2,\\cdots, n+1,\\] 即 \\[F(\\mathbf{v}^{\\ast})=\\mathbf{v}^{\\ast}.\\]Schauder不动点定理 假设$X$是Banach空间，$K\\subset X$是紧的凸集，且$A:K\\rightarrow K$是连续的，则$A$在$K$中有不动点。\nProof：\n$K$是紧集，存在有限个闭球$B(u_i;\\epsilon)$覆盖$K$。构造$K_{\\epsilon}$为${u_i}$生成的闭凸包，则$K_{\\epsilon}\\subset K$。定义 \\[P_{\\epsilon}[u]=\\frac{\\sum_{i=1}^n\\mathrm{dist}(u,K-B(u_i;\\epsilon))u_i}{\\sum_{i=1}^n\\mathrm{dist}(u,K-B(u_i;\\epsilon))}\\] 则$P_{\\epsilon}:K\\rightarrow K_{\\epsilon}$是连续函数且 \\[\\norm{P_{\\epsilon}[u]-u}\\le\\epsilon.\\] $K_{\\epsilon}$与$B(0,1)$同胚，由Brouwer不动点定理，$A_{\\epsilon}:K_{\\epsilon}\\rightarrow K_{\\epsilon}, \\ A_{\\epsilon}[u]:=P_{\\epsilon}[A[u]]$有不动点$u_{\\epsilon}\\in K$。由$K$的紧性，存在序列${u_{\\epsilon_j}}$和$u$使得$u_{\\epsilon_j}\\rightarrow u$. \\[\\norm{u_{\\epsilon_j}-A(u_{\\epsilon_j})}=\\norm{A_{\\epsilon_j}(u_{\\epsilon_j})-A(u_{\\epsilon_j})}\\le \\epsilon_j,\\] 结合$A$的连续性即得$A(u)=u$。 Schaefer不动点定理 假设$X$是Banach空间，$A:X\\rightarrow X$是连续的紧映射（有界序列的像存在$X$中的收敛子列），且集合 \\[\\{u\\in X: u=\\lambda A[u],\\text{for some }0\\le \\lambda \\le 1\\}\\] 是有界的，则$A$有不动点。\nRemark： 如果我们能在假设解存在的前提下，证明解满足某些先验估计（有界），那么这些解实际上确实存在。\nProof：\n存在充分大的$M\u0026gt;0$，使得 \\[\\norm{u}\\le M,\\quad \\text{if }u=\\lambda A[u] \\text{ for some } 0\\le \\lambda\\le 1.\\] 定义 \\[\\tilde{A}[u]:=\\begin{cases}A[u],\u0026\\text{if }\\norm{A[u]}\\le M,\\\\\\frac{MA[u]}{\\norm{A[u]}},\u0026\\text{if }\\norm{A[u]}\\ge M.\\end{cases}\\] 则$\\tilde{A}:B(0,M)\\rightarrow B(0,M)$。令$K$为$\\tilde{A}(B(0,M))$的闭凸包，由于$A$是紧映射，于是$\\tilde{A}$也是紧映射，因此$K$是紧（度量空间列紧闭$\\iff$紧）的凸集，$\\tilde{A}:K\\rightarrow K$。 由Schauder不动点定理，存在$u$，使得$\\tilde{A}[u]=u$。下面证明$u$是$A$的不动点，若不然，$u\\in K-B(0,M)$，即$\\norm{A[u]}\u0026gt;M$。于是 \\[u=\\tilde{A}[u]=\\frac{MA[u]}{\\norm{A[u]}}=\\lambda A[u], \\quad \\lambda=\\frac{M}{\\norm{A[u]}}.\\] 从而\\(\\norm{u}=M\\)，矛盾于1中假设。因此$u$是$A$的不动点。 参考文献 Partial Differential Equations. Lawrence C. Evans Banach空间和不动点定理（完）：有趣的Brouwer不动点 ","date":"2025-08-02T20:28:33+08:00","permalink":"https://baichuan-blog.netlify.app/p/fixed-point-theorems/","title":"Fixed Point Theorems"},{"content":"Piola变换 Lemma (Differential operators). 设$v\\in C^1(K)$， $\\mathbf{v}\\in \\mathbf{C}^1(K)$，$\\hat{K}$为参考单元，$\\mathbf{T}_K:\\hat{K}\\rightarrow K$为几何映射，\\(\\mathbb{J}_K=\\frac{\\p \\mathbf{T}(\\mathbf{x})}{\\p \\mathbf{\\hat{x}}}=(\\p_{j} (\\mathbf{T}_K)_i)\\)，令$\\hat{v}(\\hat{x}):=v\\circ \\mathbf{T}(\\mathbf{\\hat{\\mathbf{x}}})$，$\\hat{\\mathbf{v}}(\\hat{x}):=\\mathbf{v}\\circ \\mathbf{T}(\\mathbf{\\hat{\\mathbf{x}}})$，则下面等式关系成立： \\[\\begin{aligned} \\hat\\nabla\\hat{v}\u0026=(\\mathbb{J}_K(\\hat{\\mathbf{x}}))^T(\\nabla v)|_{\\mathbf{T}(K)(\\hat{\\mathbf{x}})},\\\\ \\hat\\nabla\\times (\\mathbb{J}_K)^T\\mathbf{\\hat{v}}\u0026=\\mathrm{det}(\\mathbb{J}_K(\\hat{\\mathbf{x}}))\\mathbb{J}_K^{-1}(\\hat{\\mathbf{x}})(\\nabla\\times \\mathbf{v})|_{\\mathbf{T}_K(\\mathbf{\\hat{x}})},\\\\ \\hat{\\nabla}\\cdot (\\mathrm{det}(\\mathbb{J}_K)\\mathbb{J}_K^{-1}\\mathbf{\\hat{v}})\u0026=\\mathrm{det}(\\mathbb{J}_K(\\hat{\\mathbf{x}}))(\\nabla\\cdot\\mathbf{v})|_{\\mathbf{T}_K(\\hat{\\mathbf{x}})}. \\end{aligned}\\]定义Piola变换如下： \\[\\begin{aligned} \u0026\\psi_K^g(v):=v\\circ \\mathbf{T}_K,\\\\ \u0026\\boldsymbol{\\psi}_K^c(\\mathbf{v}):=\\mathbb{J}_K^T(\\mathbf{v}\\circ\\mathbf{T}_K),\\\\ \u0026\\boldsymbol{\\psi}_K^d(\\mathbf{v}):=\\mathrm{det}(\\mathbb{J}_K)\\mathbb{J}_K^{-1}(\\mathbf{v}\\circ\\mathbf{T}_K),\\\\ \u0026\\psi^b_K(v):=\\mathrm{det}(\\mathbb{J}_K)(v\\circ \\mathbf{T}_K). \\end{aligned}\\] 其中$\\psi^g_K$为geometic mapping的拉回，$\\boldsymbol{\\psi}^c_K$为covariant（协变）Piola transformation，$\\boldsymbol{\\psi}^d_K$为contravariant（反变）Piola transformation。\n将前面引理的等式用Piola变换表示可得：\nCorollary (Commuting properties). \\[\\begin{aligned}\\hat\\nabla(\\psi_K^g(v))\u0026=\\boldsymbol{\\psi}_K^c(\\nabla v)\\\\ \\hat\\nabla\\times (\\boldsymbol{\\psi}_K^c(\\mathbf{v}))\u0026=\\boldsymbol{\\psi}_K^d(\\nabla\\times \\mathbf{v})\\\\ \\hat{\\nabla}\\cdot(\\boldsymbol{\\psi}_K^d(\\mathbf{v}))\u0026=\\psi_K^d(\\nabla\\cdot\\mathbf{v}) \\end{aligned}\\]法向和切向向量 Lemma (Normal and tangent).\n法向：\\[\\mathbf{n}|_{K|F}(\\mathbf{x})=\\frac{1}{\\norm{ (\\mathbb{J}_K^{-T}\\hat{\\mathbf{n}}_{\\hat{K}|\\hat{F}})(\\hat{\\mathbf{x}})}_{l^2}}(\\mathbb{J}_K^{-T}\\hat{\\mathbf{n}}_{\\hat{K}|\\hat{F}})(\\hat{\\mathbf{x}}).\\] 切向：\\[\\boldsymbol{\\tau}_E(\\mathbf{x})=\\frac{1}{\\norm{(\\mathbb{J}_K\\hat{\\boldsymbol{\\tau}}_{\\hat{E}})(\\hat{\\mathbf{x}})}_{l^2}}(\\mathbb{J}_K\\hat{\\boldsymbol{\\tau}}_{\\hat{E}})(\\hat{\\mathbf{x}}).\\] Lemma (Surface and line measure).\n面测度： \\[\\begin{aligned}\u0026\\d s=|\\mathrm{det}(\\mathbb{J}_K)(\\hat{\\mathbf{x}})|\\norm{ (\\mathbb{J}_K^{-T}\\hat{\\mathbf{n}}_{\\hat{K}|\\hat{F}})(\\hat{\\mathbf{x}})}_{l^2}\\d \\hat s,\\\\ \u0026\\d \\hat s=|\\mathrm{det}(\\mathbb{J}_K^{-1})({\\mathbf{x}})|\\norm{ (\\mathbb{J}_K^{T}{\\mathbf{n}}_{{K}|{F}})({\\mathbf{x}})}_{l^2}\\d s.\\\\ \\end{aligned}\\] 线测度： \\[\\begin{aligned}\u0026\\d l=\\norm{(\\mathbb{J}_K\\hat{\\boldsymbol{\\tau}_E})(\\hat{\\mathbf{{x}}})}_{l^2}\\d \\hat l,\\\\ \u0026\\d \\hat l=\\norm{(\\mathbb{J}^{-1}_K{\\boldsymbol{\\tau}_E})({\\mathbf{{x}}})}_{l^2}\\d l. \\end{aligned}\\] Lemma (Preservation of moments of normal and tangent). 设$\\mathbf{v}\\in \\mathbf{C}^0(K)$， $q\\in C^0(K)$，则 \\[\\begin{aligned} \u0026\\int_F (\\mathbf{v}\\cdot\\mathbf{n}|_F)(\\mathbf{x})q(\\mathbf{x})\\d s=\\int_{\\hat F} (\\boldsymbol{\\psi}^d_K(\\mathbf{v})\\cdot\\mathbf{\\hat{n}}|_{\\hat F})(\\mathbf{\\hat{x}})\\psi_K^g(q)(\\mathbf{\\hat{x}})\\d \\hat s,\\\\ \u0026\\int_E (\\mathbf{v}\\cdot\\cuti{n}|_{E})(\\mathbf{x})q(\\mathbf{x})\\d s=\\int_{\\hat E} (\\boldsymbol{\\psi}^c_K(\\mathbf{v})\\cdot\\cuti{\\hat{\\tau}}|_{\\hat E})(\\mathbf{\\hat{x}})\\psi_K^g(q)(\\mathbf{\\hat{x}})\\d \\hat l. \\end{aligned} \\]参考文献 Finite Elements I. Ch 9. ","date":"2025-08-01T23:03:37+08:00","permalink":"https://baichuan-blog.netlify.app/p/piola-transformations/","title":"Piola Transformations"},{"content":"H(div) finite elements Raviart-Thomas finite elements 多项式空间：\\(\\mathbb{RT}_{k,d} := \\mathbf{P}_{k,d}\\oplus \\mathbf{x}\\mathrm{P}^H_{k,d}\\subset \\mathbf{P}_{k+1,d}\\)，\n其中$\\mathrm{P}^H_{k,d}$是$d$元齐次多项式全体，\\(\\mathrm{dim}(\\mathrm{P}^H_{k,d})=\\begin{pmatrix}k+d-1\\\\ d-1\\end{pmatrix}\\)，\\(\\mathrm{dim}(\\mathrm{P}_{k,d})=\\begin{pmatrix}k+d\\\\d\\end{pmatrix}\\)（由组合技巧隔板法易证）。\n\\[\\begin{aligned}\\mathrm{dim}\\mathbb{RT}_{k,d} =\u0026d\\cdot\\mathrm{dim}(\\mathrm{P}_{k,d})+\\mathrm{dim}(\\mathrm{P}^H_{k,d})\\\\ =\u0026\\frac{d(k+d)!}{k!d!}+\\begin{pmatrix}k+d-1\\\\ k\\end{pmatrix}\\\\ =\u0026(k+d+1)\\begin{pmatrix}k+d-1\\\\ k\\end{pmatrix}=(k+d+1)\\begin{pmatrix}k+d-1\\\\ d-1\\end{pmatrix}. \\end{aligned}\\] 自由度：$\\forall\\mathbf{v}\\in \\mathbb{RT}_{k,d} $，\nface：\\(\\sigma_{F,j}(\\mathbf{v})=\\int_F\\mathbf{v}\\cdot \\mathbf{n}_{F}q_j\\d s,\\quad \\forall F\\in\\mathcal{F}_K,\\ q_j\\in \\mathrm{P}_{k,d-1}.\\) cell：\\(\\sigma_{K,i,j}(\\mathbf{v})=\\int_K\\mathbf{v}\\cdot \\mathbf{n}_{F_i}\\psi_j\\d \\cuti{x},\\quad \\forall i\\in\\{1,\\cdots,d\\},\\ \\psi_j\\in \\mathrm{P}_{k-1,d}.\\) \\[\\begin{aligned}\u0026\\mathrm{card}\\{\\sigma_{F,j}\\}=(d+1)\\mathrm{dim}(\\mathrm{P}_{k,d-1}) = (d+1)\\begin{pmatrix}k+d-1\\\\ d-1\\end{pmatrix}\\\\ \u0026\\mathrm{card}\\{\\sigma_{K,i,j}\\}=d\\cdot\\mathrm{dim}(\\mathrm{P}_{k-1,d})=k\\begin{pmatrix}k+d-1\\\\ d-1\\end{pmatrix}\\\\ \u0026\\mathrm{card}\\{\\sigma_{F,j}\\}+\\mathrm{card}\\{\\sigma_{K,i,j}\\}=\\mathrm{dim}\\mathbb{RT}_{k,d} \\end{aligned} \\] \\(\\forall\\mathbf{v}\\in \\mathbb{RT}_{k,d}\\)，\n\\(\\nabla\\cdot\\mathbf{v}\\in \\mathrm{P}_{k,d}\\)，\nIf \\(\\mathbf{v}\\) is divergence-free, then \\(\\mathbf{v}\\in \\mathbf{P}_{k,d}.\\)\n\\[\\nabla\\cdot(q(\\mathbf{x})\\mathbf{x}) =\\nabla q(\\mathbf{x})\\cdot\\mathbf{x}+q(\\mathbf{x})\\cdot d=(k+d)q(\\mathbf{x}),\\quad \\forall q\\in\\mathrm{P}_{k,d}^H.\\] Brezzi-Douglas-Marini elements 多项式空间：\\(\\mathbb{BDM}_{k,d}:=\\mathbf{P}_{k,d}\\subsetneq \\mathbb{RT}_{k,d}\\).\n自由度：\nface：与\\(\\mathbb{RT}_{k,d}\\)相同 cell： \\(\\sigma_{K,i,j}(\\mathbf{v})=\\int_K\\mathbf{v}\\cdot\\boldsymbol{\\psi}_j\\d \\cuti{x},\\quad \\forall \\psi_j\\in \\mathbb{N}_{k-2,d}.\\) \\[\\begin{aligned} \\mathrm{card}\\{\\sigma_{K,i,j}\\}=\u0026\\mathrm{dim}(\\mathbb{N}_{k-2,d})\\\\=\u0026\\frac{((k-2)+d+1)!}{(k-2)!(d-1)!k}\\\\ =\u0026(k-1)\\begin{pmatrix}k+d-1\\\\ d\\end{pmatrix}\\\\ \\mathrm{card}\\{\\sigma_{F,j}\\}\\ +\\ \u0026 \\mathrm{card}\\{\\sigma_{K,i,j}\\}=d\\begin{pmatrix}k+d\\\\ d\\end{pmatrix}\\\\ =\u0026\\mathrm{dim}(\\mathbf{P}_{k,d}). \\end{aligned} \\] H(curl) finite elements Nédélec finite elements 多项式空间：\\(\\mathbb{N}_{k,d}=\\mathbf{P}_{k,d}\\oplus \\mathbb{S}_{k+1,d}\\subset\\mathbf{P}_{k+2,d}\\)，\n其中\\(\\mathbb{S}_{k+1,d}:=\\{\\mathbf{q}\\in\\mathbf{P}^H_{k+1,d}:\\mathbf{q}(\\mathbf{x})\\cdot\\mathbf{x}=0\\}\\)\n\\(\\phi:\\mathbf{P}^H_{k,d}\\rightarrow \\mathrm{P}^H_{k+1,d},\\mathbf{q}\\mapsto \\mathbf{x}\\cdot \\mathbf{q}(\\mathbf{x})\\)是满射。\n\\[\\begin{aligned} \\mathrm{dim}(\\mathbb{S}_{k,d})\u0026 =\\mathrm{dim}(\\mathrm{ker}(\\phi))\\\\\u0026= \\mathrm{dim}(\\mathbf{P}^H_{k,d})-\\mathrm{dim(Im}(\\phi))\\\\ \u0026=d\\frac{(k+d-1)!}{k!(d-1)!}-\\frac{(k+d)!}{(k+1)!(d-1)!}\\\\ \u0026 = k\\frac{(k+d-1)!}{(k+1)!(d-2)!}\\end{aligned}\\] \\[\\begin{aligned}\\mathrm{dim}(\\mathbb{N}_{k,d})\u0026=d\\frac{(k+d)!}{k!d!}+(k+1)\\frac{(k+d)!}{(k+2)!(d-2)!}\\\\\u0026=\\frac{(k+d+1)!}{k!(d-1)!(k+2)}\\end{aligned}\\] 自由度：\n$d=2$，\nedge：\\(\\sigma_{E,j}(\\mathbf{v})=\\int_{E}\\mathbf{v}\\cdot\\mathbf{t}_E q_j \\d l,\\quad \\forall E\\in\\mathcal{E}_K,\\ q_j\\in \\mathrm{P}_{k,1}\\)\ncell：\\(\\sigma_{K,i,j}(\\mathbf{v})=\\int_{F}\\mathbf{v}\\cdot\\mathbf{t}_{K,i} \\psi_j \\d \\mathbf{x},\\quad \\forall i=1,2, \\ \\psi_j\\in \\mathrm{P}_{k-1,2}\\) \\[\\begin{aligned}\u0026\\mathrm{card}\\{\\sigma_{E,j}\\}+\\mathrm{card}\\{\\sigma_{E,j}\\}\\\\=\u00263(k+1)+2\\frac{(k+1)k}{2}\\\\ =\u0026(k+1)(k+3)\\\\ =\u0026\\mathrm{dim}(\\mathbb{N}_{k,d})\\end{aligned}\\]\\(\\mathbb{N}_{k,2}=R_{\\frac{\\pi}{2}}(\\mathbb{RT}_{k,2})\\)，即R-T元的旋转。\n$d=3$，\nedge：\\(\\sigma_{E,j}(\\mathbf{v})=\\int_{E}\\mathbf{v}\\cdot\\mathbf{t}_E q_j \\d l,\\quad \\forall E\\in\\mathcal{E}_K,\\ q_j\\in \\mathrm{P}_{k,1}\\) face：\\(\\sigma_{F,i,j}(\\mathbf{v})=\\int_{F}\\mathbf{v}\\cdot\\mathbf{t}_{K,i} \\psi_j \\d s,\\quad \\forall i=1,2,\\ \\psi_j\\in \\mathrm{P}_{k-1,2}\\) cell：\\(\\sigma_{K,i,j}(\\mathbf{v})=\\int_{F}\\mathbf{v}\\cdot\\mathbf{t}_{K,i} \\varphi_j \\d \\mathbf{x},\\quad \\forall i=1,2,3,\\ \\varphi_j\\in \\mathrm{P}_{k-2,3}\\) \\(\\forall \\mathbf{v}\\in\\mathbb{N}_{k,d}\\)，\n$\\nabla\\times \\mathbf{v}\\in\\mathbf{P}_{k,d}$，\nIf \\(\\mathbf{v}\\) is curl-free, then there exits $q\\in\\mathrm{P}_{k+1,d}$ such that \\(\\mathbf{v}=\\nabla q\\) and \\(\\mathbf{v}\\in\\mathbf{P}_{k,d}\\).\n无旋场是有势场，$q(\\mathbf{x})=\\int_{\\mathbf{x}_0}^{\\mathbf{x}}\\mathbf{v}\\cdot\\mathbf{t}\\d l$，参见浅水方程笔记中的场论一节[2]。因此$q$仍是多项式。\n参考文献 Finite Elements I. Ch 14-15. Shallow Water. Section 1 场论. ","date":"2025-08-01T16:40:59+08:00","permalink":"https://baichuan-blog.netlify.app/p/hdiv-and-hcurl-finite-elements/","title":"H(div) and H(curl) finite elements"},{"content":"MLP 多层感知机（multilayer Perceptron, MLP）是一种前馈人工神经网络，由多个神经元（神经节点）组成/这些神经元按照层次结构排列，包括输入层、隐藏层和输出层，层与层之间的神经元通过权重全连接，信息从输入层依次向前传播到输出层，没有反馈连接。通过反向传播更新参数。\nCNN MLP十分适合处理表格数据，其中行对应样本，列对应特征。 对于表格数据，我们寻找的模式可能涉及特征之间的交互，但是我们不能预先假设任何与特征交互相关的先验结构。 此时，多层感知机可能是最好的选择，然而对于高维感知数据，这种缺少结构的网络可能会变得不实用。 卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法。\n适合于计算机视觉的神经网络架构应具有如下性质：\n平移不变性（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。\n局部性（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。\n严格来说，卷积层是个错误的叫法，因为这里所设计的运算其实是互相关运算（cross-correlation），而不是卷积运算。\n多输入通道的互相关运算，简而言之，就是对每个通道执行互相关操作，然后将结果相加。\n汇聚（pooling，池化）层具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。通常当我们处理图像时，我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。 而我们的机器学习任务通常会跟全局图像的问题有关（例如，“图像是否包含一只猫呢？”），所以我们最后一层的神经元应该对整个输入的全局敏感。通过逐渐聚合信息，生成越来越粗糙的映射，最终实现学习全局表示的目标，同时将卷积图层的所有优势保留在中间层。\nLeNet 使用卷积层进行处理，我们可以在图像中保留空间结构。同时，卷积层代替全连接层的另一个好处是：模型更简洁、所需的参数更少。\nLeNet是最早发布的卷积神经网络之一，LeNet（LeNet-5）由两个部分组成：\n卷积编码器：由两个卷积层组成; 全连接层密集块：由三个全连接层组成。 下面两图展示了LeNet的架构：\n现代卷积神经网络 深度卷积神经网络（AlexNet） 从LeNet（左）到AlexNet（右） 使用块的网络（VGG） 经典卷积神经网络的基本组成部分是下面的这个序列：\n带填充以保持分辨率的卷积层；\n非线性激活函数，如ReLU；\n汇聚层，如最大汇聚层。\n而一个VGG块与之类似，由一系列卷积层组成，后面再加上用于空间下采样的最大汇聚层。\n从AlexNet到VGG，它们本质上都是块设计 网络中的网络（NiN） NiN块以一个普通卷积层开始，后面是两个$1\\times1$ 的卷积层。这两个$1\\times1$ 卷积层充当带有ReLU激活函数的逐像素全连接层。\n对比 VGG 和 NiN 及它们的块之间主要架构差异 NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。 相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个全局平均汇聚层（global average pooling layer），生成一个对数几率 （logits）。NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。\n小结：\nNiN使用由一个卷积层和多个 卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。\nNiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。\n移除全连接层可减少过拟合，同时显著减少NiN的参数。\nNiN的设计影响了许多后续卷积神经网络的设计。\n含并行连结的网络（GoogLeNet） 在GoogLeNet中，基本的卷积块被称为Inception块（Inception block）。这很可能得名于电影《盗梦空间》（Inception），因为电影中的一句话“我们需要走得更深”（“We need to go deeper”）。\nInception块的架构 GoogLeNet架构 小结：\nInception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用 卷积层减少每像素级别上的通道维数从而降低模型复杂度。\nGoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。\nGoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。\n批量规范化 训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。 本节将介绍批量规范化（batch normalization），这是一种流行且有效的技术，可持续加速深层网络的收敛速度。 再结合在下一小节中将介绍的残差块，批量规范化使得研究人员能够训练100层以上的网络。\n$$\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.$$残差网络（ResNet） 一个正常块（左图）和一个残差块（右图） 左图虚线框中的部分直接拟合映射$f(\\mathbf{x})$，而右图则是拟合出残差映射$f(\\mathbf{x}) - \\mathbf{x}$。\nResNet-18 架构 小结：\n学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。\n残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。\n利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。\n残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。\n稠密连接网络（DenseNet） ResNet将 分解为两部分：一个简单的线性项和一个复杂的非线性项。 那么再向前拓展一步，如果我们想将 拓展成超过两部分的信息呢？ 一种方案便是DenseNet。\n小结：\n在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。\nDenseNet的主要构建模块是稠密块和过渡层。\n在构建DenseNet时，我们需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量。\nRNN 如果说卷积神经网络可以有效地处理空间信息， 那么循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。\nPreliminaries 隐变量自回归模型： 一个经典方法是使用历史观测来预测下一个未来观测。 显然，我们并不指望时间会停滞不前。 然而，一个常见的假设是虽然特定值 可能会改变， 但是序列本身的动力学不会改变。 这样的假设是合理的，因为新的动力学一定受新的数据影响， 而我们不可能用目前所掌握的数据来预测新的动力学。 统计学家称不变的动力学为静止的（stationary）。 因此，整个序列的估计值都将通过以下的方式获得： $$P(x_1,x_2,\\cdots,x_T)=\\prod_{t=1}^TP(x_t|x_{t-1},\\cdots,x_1).$$ 一阶马尔可夫模型： $$P(x_1,x_2,\\cdots,x_T)=\\prod_{t=1}^TP(x_t|x_{t-1}),\\quad P(x_1|x_0)=P(x_1).$$ 循环神经网络（recurrent neural networks，RNNs） 是具有隐状态的神经网络。下图展示了循环神经网络在三个相邻时间步的计算逻辑。 在任意时间步，隐状态的计算可以被视为：\n拼接当前时间步$t$的输入$X_t$和前一时间步$t-1$的隐状态$H_{t-1}$。 将拼接的结果送入带有激活函数$\\phi$的全连接层。 全连接层的输出是当前时间步$t$的隐状态$H_t$。 具有隐变量的循环神经网络 当前时间步隐藏层计算公式： $$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h),\\tag{1}$$ 输出层输出： $$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$ 从相邻时间步的隐藏变量$H_t$和$H_{t-1}$之间的关系可知， 这些变量捕获并保留了序列直到其当前时间步的历史信息， 就如当前时间步下神经网络的状态或记忆， 因此这样的隐藏变量被称为隐状态（hidden state）。 由于在当前时间步中， 隐状态使用的定义与前一个时间步中使用的定义相同， 因此 (1)的计算是循环的（recurrent）。 于是基于循环计算的隐状态神经网络被命名为 循环神经网络（recurrent neural network）。 在循环神经网络中执行 (1)计算的层 称为循环层（recurrent layer）。\n现代循环神经网络 门控循环单元（Gate reccurent unit，GRU） 门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。 这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。 重置门$\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$和更新门$\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$的计算如下所示： $$\\begin{aligned} \\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r),\\\\ \\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z), \\end{aligned}$$ 在门控循环单元模型中计算重置门和更新门 接下来，让我们将重置门与 (1) 中的常规隐状态更新机制集成， 得到在时间步$t$的候选隐状态（candidate hidden state） \\(\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}\\)： $$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h).\\tag{2}$$ 这里，我们使用\\(\\tanh\\)非线性激活函数来确保候选隐状态中的值保持在区间$(-1,1)$中。 与 (1)相比， (2)中的\\(\\mathbf{R}_t\\) 和$\\mathbf{H}_{t-1}$ 的元素相乘可以减少以往状态的影响。 每当重置门\\(\\mathbf{R}_t\\) 中的项接近1 时， 我们恢复一个如 (1)中的普通的循环神经网络。 对于重置门$\\mathbf{R}_t$ 中所有接近0 的项， 候选隐状态是以$\\mathbf{X}_t$ 作为输入的多层感知机的结果。 因此，任何预先存在的隐状态都会被重置为默认值。\n在门控循环单元模型中计算候选隐状态 上述的计算结果只是候选隐状态，下面我们结合更新门$\\mathbf{Z}_t$的效果得到门控循环单元的最终更新公式： $$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1} + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$ 每当更新门$\\mathbf{Z}_t$ 接近1 时，模型就倾向只保留旧状态。 此时，来自$\\mathbf{X}_t$ 的信息基本上被忽略， 从而有效地跳过了依赖链条中的时间步$t$ 。 相反，当$\\mathbf{Z}_t$ 接近0 时， 新的隐状态$\\mathbf{H}_t$ 就会接近候选隐状态$\\tilde{\\mathbf{H}}_t$ 。 这些设计可以帮助我们处理循环神经网络中的梯度消失问题， 并更好地捕获时间步距离很长的序列的依赖关系。\n计算门控循环单元模型中的隐状态 小结：\n门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。\n重置门有助于捕获序列中的短期依赖关系。\n更新门有助于捕获序列中的长期依赖关系。\n重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。\n长短期记忆网络（Long short-term memory，LSTM） 长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。 解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）。\n$$\\begin{split}\\begin{aligned} 输入门:\\mathbf{I}_t \u0026= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\ 遗忘门:\\mathbf{F}_t \u0026= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\ 输出门:\\mathbf{O}_t \u0026= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\\\\ 候选记忆: \\tilde{\\mathbf{C}}_t \u0026= \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),\\\\ 记忆: \\mathbf{C}_t \u0026= \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t,\\\\ 隐变量:\\mathbf{H}_t \u0026= \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t). \\end{aligned}\\end{split}$$长短期记忆网络是典型的具有重要状态控制的隐变量自回归模型。 多年来已经提出了其许多变体，例如，多层、残差连接、不同类型的正则化。 然而，由于序列的长距离依赖性，训练长短期记忆网络 和其他序列模型（例如门控循环单元）的成本是相当高的。 在后面的内容中，我们将讲述更高级的替代模型，如Transformer。\n小结：\n长短期记忆网络有三种类型的门：输入门、遗忘门和输出门。\n长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。\n长短期记忆网络可以缓解梯度消失和梯度爆炸。\n深度循环神经网络 深度循环神经网络架构 双向循环神经网络 很明显，每个短语的“下文”传达了重要信息（如果有的话）， 而这些信息关乎到选择哪个词来填空， 所以无法利用这一点的序列模型将在相关任务上表现不佳。 例如，如果要做好命名实体识别 （例如，识别“Green”指的是“格林先生”还是绿色）， 不同长度的上下文范围重要性是相同的。\n双向神经网络架构 双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。 也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。 但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。 因为在预测下一个词元时，我们终究无法知道下一个词元的下文是什么， 所以将不会得到很好的精度。 具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词； 而在测试期间，我们只有过去的数据，因此精度将会很差。\n另一个严重问题是，双向循环神经网络的计算速度非常慢。 其主要原因是网络的前向传播需要在双向层中进行前向和后向递归， 并且网络的反向传播还依赖于前向传播的结果。 因此，梯度求解将有一个非常长的链。\n双向层的使用在实践中非常少，并且仅仅应用于部分场合。 例如，填充缺失的单词、词元注释（例如，用于命名实体识别） 以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）。\n序列到序列学习 机器翻译中的输入序列和输出序列都是长度可变的， 为了解决这类问题，我们考虑“编码器——解码器”架构。遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入， 将其转换为固定形状的隐状态。 换言之，输入序列的信息被编码到循环神经网络编码器的隐状态中。 为了连续生成输出序列的词元， 独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元。\n使用循环神经网络编码器和循环神经网络解码器的序列到序列学习 其中特定的“”表示序列结束词元。 一旦输出序列生成此词元，模型就会停止预测。 在循环神经网络解码器的初始化时间步，有两个特定的设计决定： 首先，特定的“”表示序列开始词元，它是解码器的输入序列的第一个词元。 其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。上图中编码器最终的隐状态在每一个时间步都作为解码器的输入序列的一部分。\n小结：\n根据“编码器-解码器”架构的设计， 我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。\n在实现编码器和解码器时，我们可以使用多层循环神经网络。\n我们可以使用遮蔽来过滤不相关的计算，例如在计算损失时。\n在“编码器－解码器”训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。\nBLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的 元语法的匹配度来评估预测。\n序列搜索策略 序列搜索策略包括贪心搜索、穷举搜索和束搜索。\n贪心搜索所选取序列的计算量最小，但精度相对较低。\n穷3. 举搜索所选取序列的精度最高，但计算量最大。\n束搜索通过灵活选择束宽，在正确率和计算代价之间进行权衡。 注意力机制 注意力提示 “是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。 在注意力机制的背景下，自主性提示被称为查询（query）。 给定任何查询，注意力机制通过注意力汇聚（attention pooling） 将选择引导至感官输入（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为值（value）。 更通俗的解释，每个值都与一个键（key）配对， 这可以想象为感官输入的非自主提示。 如下图所示，可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。\n注意力机制通过注意力汇聚将查询（自主性提示）和键（非自主性提示）结合在一起，实现对值（感官输入）的选择倾向 注意力汇聚: Nadaraya-Watson 核回归 注意力汇聚公式： $$f(x) = \\sum_{i=1}^n \\alpha(x, x_i) y_i,$$ 其中$x$是查询（query），$(x_i,y_i)$是键值对（key-value）。 注意力汇聚是$y_i$的加权平均。 将查询$x$和键$x_i$ 之间的关系建模为注意力权重（attention weight）$\\alpha(x,x_i)$。对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布： 它们是非负的，并且总和为1。\nNadaraya-Watson核回归将注意力权重定义为： $$\\alpha(x,x_i) = \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} ,$$ 其中$K$是核，我们可考虑高斯核： $$K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2}).$$小结：\nNadaraya-Watson核回归是具有注意力机制的机器学习范例。\nNadaraya-Watson核回归的注意力汇聚是对训练数据中输出的加权平均。从注意力的角度来看，分配给每个值的注意力权重取决于将值所对应的键和查询作为输入的函数。\n注意力汇聚可以分为非参数型和带参数型。\n注意力评分函数 上节使用了高斯核来对查询和键之间的关系建模， 其中的高斯核指数部分可以视为注意力评分函数（attention scoring function）， 简称评分函数（scoring function）。Nadaraya-Watson核回归的注意力汇聚权重$\\alpha(x,x_i)$可以看成是对评分函数的输出结果做一个softmax运算。\n计算注意力汇聚的输出为值的加权和 加性注意力： 一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。 给定查询$\\mathbf{q} \\in \\mathbb{R}^q$ 和 键$\\mathbf{k} \\in \\mathbb{R}^k$， 加性注意力（additive attention）的评分函数为 $$a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},$$ 其中可学习的参数是$\\mathbf W_q\\in\\mathbb R^{h\\times q}$、 $\\mathbf W_k\\in\\mathbb R^{h\\times k}$和$\\mathbf w_v\\in\\mathbb R^{h}$。 即将查询和键连结起来后输入到一个多层感知机（MLP）中， 感知机包含一个隐藏层，其隐藏单元数是一个超参数$h$，使用$\\tanh$作为激活函数，并且禁用偏置项。\n多头注意力 与其只使用单独一个注意力汇聚，我们可以先对查询、 键和值做多组不同的可学习的线性变换，再对变换后的多组查询、 键和分别进行注意力汇聚，最后通过可学习的线性变换将这多组注意力的输出进行连结。这种设计被称为多头注意力（multihead attention）。\n多头注意力：多个头连结然后线性变换 小结：\n多头注意力融合了来自于多个注意力汇聚的不同知识，这些知识的不同来源于相同的查询、键和值的不同的子空间表示。\n基于适当的张量操作，可以实现多头注意力的并行计算。\n自注意力和位置编码 比较卷积神经网络（填充词元被忽略）、循环神经网络和自注意力三种架构 小结：\n在自注意力中，查询、键和值都来自同一组输入。\n卷积神经网络和自注意力都拥有并行计算的优势，而且自注意力的最大路径长度最短。但是因为其计算复杂度是关于序列长度的二次方，所以在很长的序列中计算会非常慢。\n为了使用序列的顺序信息，可以通过在输入表示中添加位置编码，来注入绝对的或相对的位置信息。\nTransformer transformer架构 参考文献 动手学深度学习 ","date":"2025-07-18T09:20:29+08:00","permalink":"https://baichuan-blog.netlify.app/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"动手学深度学习"},{"content":"Preliminaries 在一个机器学习模型的训练中，有几个基础的组成部分：\nData：数据集，可以做一些前处理来保持某些性质。 Model architecture：模型架构，可以嵌入一些先验的物理知识和指导经验。 Loss functions：损失函数/优化目标，可以利用给定的ODE/PDE/SDEs设计更好的损失函数或加入正则化项。 Optimaization algorithms：优化算法，设计合适的优化方法来增强稳定性或提高收敛速度。 interference：推断，可以将物理约束加入到推断算法中，比如加入后处理步。 Neural Solver 相较于传统的FEM、FVM等方法，神经网络求解器有两个潜在的优势：\n可以灵活地结合数据和知识。 更高效地表示高维函数。 缺点则是计算效率、精度、收敛性等方面存在一些问题。\n在neural solver中，最有代表性的莫过于Physics-Informed Nerual Networks (PINNs)，其实就是将物理信息结合到上节提到的神经网络学习中的基础组成部分中的一个或多个。\nData Data Re-Sampling：从误差更大的区域选择更多配置点(collacation points)。\nNeural Architectures 前处理(嵌入embedding)： 例如用Fourier特征由输入坐标获得嵌入向量，可以更容易学到高频信息。 多神经网络：用多个神经网络学习不同函数，如对于Dirichlet边值问题 $$\\begin{cases}\\mathcal{F}(u)(\\mathbf{x})=0, \u0026x\\in\\Omega\\\\u(\\mathbf(x))=g(\\mathbf{x}),\u0026x\\in\\p \\Omega\\end{cases},$$ 将解分解为两个部分 $$u(\\mathbf{x}) = v(\\mathbf{x})+D(\\mathbf{x})y(\\mathbf{x}),$$ 其中$D(\\mathbf{x})$再区域边界为0 (如距离函数$\\min_{\\mathbf{x}_b\\in\\p \\Omega}\\norm{\\mathbf{x}-\\mathbf{x}_b}_2$)，可以用一个神经网络学习；$v(\\mathbf{x})$满足边界条件，用第二个神经网络学习；最后利用PDE残量学习$y(\\mathbf{x})$。 序列神经网络：对于带时序的问题如非定常PDE，可以考虑Recurrent Neural Networks (RNN), Long-Short Term Memeory (LSTM), Gate Recurrent Unit (GRU), Transformer等架构。 区域分解：类似区域分解算法的想法，将区域划分为有overlapping的子区域，在每个子区域上求解子优化问题，区域交界处加一些惩罚项进行约束。 Loss function 损失函数的组成一般可以包含PDE residuals、初边值条件、数据集等。\nLoss Re-Weighting：损失函数各部分可以通过构造合适的权重组合到一起。\n一些新奇的目标函数设置：\n结合数值微分： 用数值微分替代一些自动微分项。\n变分形式：\n对于自伴微分算子如$-\\Delta$而言，常常可以写成等价的变分形式： $$\\min_{w}\\mathcal{J}(w).$$ 对于一般的问题，可以类似Petrov-Galerkin的想法，选一个测试函数集$V_K$，构造损失函数： $$\\mathcal{J}(w) = \\frac{1}{K}\\sum_{k=1}^K|\\langle\\mathcal{F}(u_w)(\\mathbf{x}),v\\rangle_{\\Omega}|^2,$$若有边值条件等则再往损失函数中添加相关项。\nWeak Adversarial Networks (WAN) 将变分形式表示成了min-max问题： $$\\min_w\\max_{\\theta}\\frac{\\langle\\mathcal{F}(u),v_{\\theta}\\rangle^2}{\\norm{v_{\\theta}}_{\\Omega}^2}.$$ 正则化项：$L$-2正则化缓解过拟合现象，$L$-1正则化可以提取稀疏特征，也可以将PDE残差的高阶导项加入正则化项。\nNerual Operator 算子学习方法主要可分为四类： DeepONet、Green\u0026rsquo;s function learning、 grid-based operator learniing和graph-based operator learning。\nNerual Operator和前面Neural solver的区别在于算子学习的目标是为ODEs/PDEs学一个代理模型而不是求解具体实例。这也意味着算子学习的输入除了坐标$\\mathbf{x}$外还可能包含初边值等条件。\nDeepONet 直接将参数$\\theta$(初边值等条件)和$\\mathbf{x}$作为训练的输入。 $$G_w(\\theta)(\\mathbf{x}) = b_0+\\sum_{k=1}^p b_k(\\theta)t_k(\\mathbf{x}),$$ 用两个神经网络分别学习$(b_1,\\cdots,b_p)$和$(t_1,\\cdot,t_p)$，其中神经网络是可以是FNNs、ResNets或其他架构。\nGreen\u0026rsquo;s Function learning 考虑线性问题如下： $$\\begin{cases}\\mathcal{F}_L(u) = f,\u0026\\mathbf{x}\\in\\Omega\\\\\\mathcal{B}_L(u) = g, \u0026\\mathbf{x}\\in\\p \\Omega\\end{cases},$$ 其中$\\mathcal{F}_L$和$\\mathcal{B}_L$都是线性算子。类似Laplace方程的Green函数法，我们希望找到Green函数\\(\\mathcal{G}(\\mathbf{x},\\mathbf{y})\\) 和\\(u_{homo}\\)使得： $$\\begin{cases}\\mathcal{F}_L(\\mathcal{G}(\\mathbf{x},\\mathbf{y})) = \\delta(\\mathbf{y}-\\mathbf{x}),\u0026\\mathbf{x},\\mathbf{y}\\in\\Omega\\\\\\mathcal{B}_L(\\mathcal{G}(\\mathbf{x},\\mathbf{y})) = 0, \u0026\\mathbf{x}\\in\\p \\Omega\\end{cases},\\quad \\begin{cases}\\mathcal{F}_L(u_{homo}(\\mathbf{x})) = 0,\u0026\\mathbf{x},\\mathbf{y}\\in\\Omega\\\\\\mathcal{B}_L(u_{homo}(\\mathbf{x})) = 0, \u0026\\mathbf{x}\\in\\p \\Omega\\end{cases}.$$ 此时解可以表示为： $$u(\\mathbf{x}) = \\int_{\\Omega}\\mathcal{G}(\\mathbf{x},\\mathbf{y})f(\\mathbf{y})\\d \\mathbf{y}+u_{homo}(\\mathbf{x}).$$对于非线性问题，可以学习$u,f$到$v,h$的映射，其中$v,h$满足线性模型，对线性模型用Green函数法。\nGrid-based Operator Learning 如果配置点取的是均匀网格上的顶点，学习算子 $$\\tilde{G}:\\theta=\\{v(\\mathbf{x}_i)\\}_{i=1}^N\\mapsto \\{u(\\mathbf{x}_i)\\}_{i=1}^N.$$ 此时\\(\\{v(\\mathbf{x}_i)\\}_{i=1}^N\\)和\\(\\{u(\\mathbf{x}_i)\\}_{i=1}^N\\)可以表示成高维张量(因为网格是规则区域的均匀剖分)，因此$\\tilde{G}$可以看成image到image的映射，可以考虑convolutional nerual networks (CNN)等处理图像的方法，也可以考虑应用注意力机制(attention mechanism)。\n缺点：规则网格，维数灾难。\nGraph-based Operator Learning Grid-based方法中的网格\\(\\{\\mathbf{x}_i\\}_{i=1}^N\\) 其实可以用图论中的图\\(\\mathcal{G}(\\mathcal{V},\\mathcal{E})\\) 来表示。Our goal is to learn the latent operator G in equation defined above in a data-driven manner.\nGraph nerual operators：Inspired by the format of Green\u0026rsquo;s function, Li et al. have introduced a graph kernel network into operator learning. The model can be described as $$\\begin{aligned} \u0026z_0(\\mathbf{x}) = P(\\mathbf{x},v(\\mathbf{x}),v_{\\epsilon}(\\mathbf{x}),\\nabla v_{\\epsilon}(\\mathbf{x}))+p,\\\\ \u0026z_{t+1}(\\mathbf{x}) = \\sigma(Wz_t(\\mathbf{x}))+\\int_{\\Omega}\\kappa_{\\phi}(\\mathbf{x},\\mathbf{y},v(\\mathbf{x}),v(\\mathbf{y}))z_t(\\mathbf{y})\\nu_x(\\d\\mathbf{y})),\\\\ \u0026u(\\mathbf{x})=Qz_T(\\mathbf{x})+q. \\end{aligned}$$参考文献 Physics-Informed Machine Learning A Survey on Problems, Methods and Applications. Hao, et al. . 2023 ","date":"2025-07-15T23:50:17+08:00","permalink":"https://baichuan-blog.netlify.app/p/physics-informed-machine-learning/","title":"Physics-Informed Machine Learning"},{"content":"The Finite Neuron Method and Convergence Analysis Some remarks The general construction of $H^m$-conforming elements in any dimension is still an open problem. This paper studies $H^m$-conforming functions based on artificial nerual network. With ReLU$^k$ as activation function, there\u0026rsquo;s a family of $H^m$-conforming piecewise polynomials.\nFor finite neuron method, the underlying finite element grids are not given a priori and the discrete solution can only be obtained by solving a nonlinear and non-convex optimization problem. On the one hand, finite nerual method gains more flexibility. But on the other hand the highly nonlinear and non-convex problem will be expensive and challenging for us to solve.\nIf we relax the conformity, it is possible to give a universal construction of convergent $H^m$-nonconforming finite element consisting of piecewise polynomial of degree $m$. In the finite neuron method setting, by relaxing the constraints from the a priori given finite element grid, the construction of $H^m$-conforming piecewise polynomials of degree $m$ becomes straightforward. In fact, the finite neuron method can be considered as mesh-less method, or even, vertex-less method although there is a hidden grid for any finite neuron function. This raises a question if it is possible to develop some \u0026ldquo;in-between\u0026rdquo; method that have the advantages of both the classic finite element method and the finite neuron method.\nIn deep neural networks, functions can be written as: $$ f(x) = \\theta^l\\circ\\sigma\\circ\\theta^{l-1}\\circ \\sigma\\cdots\\circ\\theta^1\\circ \\theta^{0}(x), $$ where $\\theta^i:\\mathbb{R}^{n_i}\\to \\mathbb{R}^{n_{i+1}}$ are linear functions with form $w_i\\cdot x+b_i$, and $\\sigma$ is the activation function.\nRELU-DNN is simply piecewise linear functions. Any linear finite element function on proper simplicial finite element grid can be written as a ReLU-DNN with at most $O(d)$ layers ($d$ is the dimension of the problem, see Lemma 4.3). So the finite neuron method has both local and global adaptive features.\nWhen choose ReLU$^k$ as activation function, the finite neuron functions set (not necessarily a linear space) contains a $\\mathbb{P}_q$ (all polynomials with degree not larger than $q$) as a subspace and the inequality in Theorem 4.2: $$\\inf_{v_N\\in_n\\mathcal{N}^k_l(N)}\\norm{u-v_N}_{H^m(\\Omega)}\\lesssim \\inf_{v_N\\in\\mathbb{P}_{k^l}}\\norm{u-v_N}_{H^m(\\Omega)}$$ indicates that deep finite neuron function may provide spetral approximate accuracy.\nThe error estimates in this paper are not sharp enough, because the results are independent of the index $k$ of ReLU$^k$.\nDNN is known to have much less \u0026ldquo;curse of dimesionality\u0026rdquo; than the traditional functional classes may be due to its overparameterization feature.\nThe core idea in the convergence analysis Monte-Carlo and stratified (分层的) sampling both derive a variance inequality. For Monte-Carlo sampling, it\u0026rsquo;s $$\\mathbb{E}_N\\left(\\mathbb{E}(g)-\\frac1{N}\\sum_{i=1}^Ng(w_i)\\right)^2=\\begin{cases}\\frac1N\\mathbb{V}(g)\\le \\frac1N\\sup_{w,w'\\in G}|g(w)-g(w')|^2\\\\\\frac1N\\left(\\mathbb{E}(g^2)-\\mathbb{E}(g)^2\\right)\\le \\frac1N \\mathbb{E}(g^2)\\le \\frac1N\\norm{g}^2_{L^{\\infty}}.\\end{cases}\\tag{1}$$Consider $$u(x)=\\int_{G}g(x,\\theta)\\rho(\\theta)\\d\\theta=\\norm{\\rho}_{L^1(G)}\\int_Gg(x,\\theta)\\frac{\\rho(\\theta)}{\\norm{\\rho}_{L^1(G)}}\\d \\theta,$$ then from the above inequality, we have an existence result in Lemma 2.3 that there exists $\\theta_i^*\\in G$ such that $$\\norm{u-u_N}_{H^m(\\Omega)}\\le \\frac{\\norm{\\rho}_{L^1(G)}}{N}\\mathbb{E}(\\norm{g(\\cdot,\\theta)}^2_{H^m(\\Omega)}),\\tag{2}$$ where $$u_N = \\frac{\\norm{\\rho}_{L^1(G)}}{N}\\sum_{i=1}^Ng(x,\\theta_i^*).\\tag{3}$$ Otherwise the variance inequality would not be true.\nThe core idea is:\nRepresent $u$ as an expectation of some probability distribution. A simple application of Monte-Carlo sampling then leads to error estimate. For example, consider the Fourier transform of a real function $u:\\mathbb{R}^d\\to \\mathbb{R}$ $$\\hat{u}(w) = (2\\pi)^{-\\frac{d}2}\\int_{\\mathbb{R}^d} e^{-iw\\cdot x}u(x)\\d x.$$ This gives the following representation of $u$ in terms of the cosine function $$u(x)=Re\\int_{\\R^d}e^{iw\\cdot x}\\hat{u}(w)\\d w=\\int_{\\R^d}\\cos(w\\cdot x +b(w))|\\hat{u}(w)|\\d w,$$ where $\\hat{u}(w) = e^{ib(w)}|\\hat{u}(w)|$. Let $$g(x,w)= \\cos(w\\cdot x +b(w))\\quad \\text{and} \\quad\\rho(w) = |\\hat{u}(w)|, $$ then $$u(x) = \\int_{\\R^d}g(x,w)\\rho(w)\\d w.$$With Monte-Carlo sampling Lemma, there exits $w_i\\in\\R^d$ such that $$\\norm{u-u_N}_{H^m(\\Omega)}\\lesssim N^{-\\frac12}\\int_{\\R^d}(1+\\norm{w})^m|\\hat{u}(w)|\\d w,\\tag{4}$$ where $$u_N = \\frac{\\|\\hat{u}\\|_{L^1(\\mathbb{R}^d)}}{N}\\sum_{i=1}^N\\cos(w_i\\cdot x+b(w_i)).$$ In this case, cosine is the activation function.\nBarron spetral space Given $v\\in L^2(\\Omega)$, consider all possible extension $v_E:\\R^d\\to \\R$ with $v_E|_{\\Omega} = v$ and define the Barron spetral norm for any $s\\ge1$: $$\\norm{v}_{B^s(\\Omega)} := \\inf_{v_E|{\\Omega=v}}\\int_{\\R^d}(1+\\norm{w})^s|\\hat{v}_E(w)|\\d w$$ and Barron spetral space $$B^s(\\Omega):=\\{v\\in L^2(\\Omega):\\norm{v}_{B^s(\\Omega)}\u003c\\infty\\}.$$With these definition, (4) can be rewritten as $$\\norm{u-u_N}_{H^m(\\Omega)}\\lesssim N^{-\\frac12}\\norm{u}_{B^s(\\Omega)}.$$Lemma 2.5 demonstrates that for any Schwarz function $v$, we have $$\\norm{u}_{H^m(\\Omega)}\\lesssim \\norm{v}_{B^s(\\Omega)} \\norm{v}_{H^{m+\\frac{d}2+\\epsilon}(\\Omega)},$$ where $m\\ge 0$ is an integer, $\\Omega\\subset \\R^d$ is a bounded domain and $\\epsilon$ is any positive real number.\nApproximation Rates for Shallow ReLU$^k$ Neural Networks on Sobolev Spaces via the Radon Transform Space embedding result (Theorem 1): $$W^{s,2}(\\Omega)\\subset \\mathcal{H}(\\mathbb{P}_k^d),$$ where $s=\\frac{d+2k+1}2$. Approximation inequality (Corollary 1): $$\\inf_{f_n\\in\\Sigma_n^k(\\R^d)}\\norm{f-f_n}_{L^{\\infty}(\\Omega)}\\le C\\norm{f}_{W^{s,2}(\\Omega)}n^{-\\frac{s}{d}},$$ where $s=\\frac{d+2k+1}2$. Generalization of 2: $$\\inf_{f_n\\in\\Sigma_n^k(\\R^d)}\\norm{f-f_n}_{L^{p}(\\Omega)}\\le C\\norm{f}_{W^{s,p}(\\Omega)}n^{-\\frac{s}{d}},$$ where $2\\le p\\le \\infty$ and $0\\le s\\le \\frac{d+2k+1}2$. 参考文献 The Finite Neuron Method and Convergence Analysis. Xu. 2020 Approximation Rates for Shallow ReLU$^k$ Neural Networks on Sobolev Spaces via the Radon Transform. Xu, et al. . 2024 ","date":"2025-07-14T22:09:35+08:00","permalink":"https://baichuan-blog.netlify.app/p/finite-neuron-method/","title":"Finite Neuron Method"},{"content":"Wachspress coordinates 对于三角形上的任何一个点，我们可以使用三个顶点的凸组合唯一表示该点，即重心坐标。但是对于凸多边形（如矩形）而言，一般情况下凸组合并不唯一。例如图中的点$v$，既在$\\triangle v_1v_{i-1}v_i$内部，也在$\\triangle v_{i-1}v_iv_{i+1}$内部，因此既可以用点$v_0,v_{i-1},v_i$的凸组合，也可以用点$v_{i-1},v_i,v_{i+1}$的凸组合表示。\n下面将介绍一种凸多边形内点的坐标表示：Wachspress coordinates。\n首先定义$\\triangle v_{i-1}v_iv_{i+1}$面积为： \\[A_i=Area(\\triangle v_{i-1}v_iv_{i+1})\\] 定义以$vv_i$为公共边的两个相邻三角形面积分别为： \\[D_{i-1} = Area(\\triangle vv_{i-1}v_i),\\quad D_{i}= Area(\\triangle vv_{i}v_{i+1})\\] 构造权函数： \\[w_i = \\frac{A_i}{D_{i-1}D_i}\\] $w_i$衡量了衡量点$v$靠近点$v_i$的程度：当$v$往$v_i$靠近时，$D_{i-1}D_i$减小（不严格），$w_i$变大；反之，当$v$远离$v_i$时，$w_i$减小。\n最后对$w_i$进行归一化： \\[\\lambda_i = \\frac{w_i}{\\sum_{i=1}^N w_i},\\quad i=1,\\cdots N\\] 由于$\\lambda_i\\ge0$且$\\sum_{i=1}^N \\lambda_i = 1$，因此${\\lambda_i}_{i=1}^N$构成了一组凸组合系数，称为Wachspress coordinates。\n当$v$恰好为某个顶点，如$v_j$时，$\\lambda_i=\\delta_{ij}$。 此时$\\sum_{i=1}^N\\lambda_iv_i = v_j=v$。\n下面我们证明对一般的$v$，也有： \\[v = \\sum_{i=1}^N\\lambda_iv_i\\]由三角形上重心坐标表示，可以将点$v$表示为$v_{i-1},v_{i},v_{i+1}$的线性组合如下： \\[v = \\frac{D_i}{A_i}v_{i-1}+\\frac{A_i-D_i-D_{i-1}}{A_i}{v_i}+\\frac{D_{i-1}}{A_i}v_{i+1}\\] 于是 $$ \\begin{aligned} \u0026w_i (v_i-v) = \\frac{1}{D_{i-1}}(v_{i}-v_{i-1})-\\frac{1}{D_i}(v_{i+1}-v_{i})\\\\ \\implies \u0026 \\sum_{i=1}^Nw_i (v_i-v) = 0\\\\ \\implies \u0026 \\sum_{i=1}^N \\lambda_i (v_i-v) = 0\\\\ \\implies \u0026 \\sum_{i=1}^N \\lambda_i v_i=v \\end{aligned} $$综上，Wachspress coordinates满足如下性质：\n非负性：$\\lambda_i\\ge 0,\\ i=1,\\cdots, N$。 凸组合：$\\sum_{i=1}^N \\lambda_i = 1$。 精确插值性：$v = \\sum_{i=1}^{N}\\lambda_i(v)v_i$。 凸多边形上的插值 在实际应用，我们经常面临这样的问题：已知函数在单元（三角形、四边形等）顶点的取值，希望估计其在单元内某一点处函数值，即利用顶点值插值得到内部点函数值。假设函数$f$在顶点$v_i$处取值为$f_i$，利用上面的Wachspress coordinates，我们可以定义在点$v$处插值： \\[\\mathbb{I}_f(v): = \\sum_{i=1}^N\\lambda_i(v)f_i\\] 当$f$是线性多项式时， \\[f(v) = f(\\sum_{i=1}^N \\lambda_iv_i )=\\sum_{i=1}^N\\lambda_i(v)f_i =\\mathbb{I}_f(v)\\] 即$f = \\mathbb{I}_f$。 当$f$为一般光滑函数时，对$f$进行一阶Taylor展开：$f = P_1(f)+O(h^2)$，$h$为单元直径。于是， $$\\begin{aligned}\\mathbb{I}_f(v) \u0026= \\sum_{i=1}^N\\lambda_i f_i \\\\ \u0026= \\sum_i \\lambda_iP_1(f)(v_i)+O(h^2) \\\\ \u0026 = P_1(f)(v)+O(h^2)\\quad (P_1\\text{是线性函数})\\\\ \u0026 = f(v)+O(h^2) \\end{aligned}$$ 即$\\norm{f-\\mathbb{I}_f}=O(h^2)$，更具体地，有$\\norm{f-\\mathbb{I}_f}\\le C\\norm{D^2f}h^2$。 参考文献 Wachspress and mean value coordinates ","date":"2025-06-20T10:56:39+08:00","permalink":"https://baichuan-blog.netlify.app/p/wachspress-coordinates/","title":"Wachspress Coordinates"},{"content":" Assemble bilinear form and get HypreParMatrix\n1 2 3 4 5 6 7 8 9 ParBilinearForm *a = new ParBilinearForm(fespace); a-\u0026gt;AddDomainIntegrator(new ConvectionIntegrator(velocity, -1.0)); // Assemble the matrix in parallel a-\u0026gt;Assemble(); a-\u0026gt;Finalize(); HypreParMatrix *A = a-\u0026gt;ParallelAssemble(); delete a; Automatic Differetiation\nMesh connectivity\nMesh Method Dimension Mesh object owns data const Table \u0026amp;ElementToElementTable() 1D, 2D, 3D Yes const Table \u0026amp;ElementToFaceTable() 1D, 2D, 3D Yes const Table \u0026amp;ElementToEdgeTable() 1D, 2D, 3D Yes Table *GetFaceEdgeTable() 3D Yes Table *GetEdgeVertexTable() 1D, 2D, 3D Yes Table *GetVertexToElementTable() 1D, 2D, 3D No Table *GetFaceToElementTable() 1D, 2D, 3D No 遍历每个单元，并对各边做处理：\n1 2 3 4 5 6 7 8 9 10 11 12 const Table \u0026amp;elem_edge = mesh.ElementToEdgeTable(); int num_elems = mesh.GetNE(); for (int elem_id = 0; ei \u0026lt; num_elems; elem_id++) { int num_edges = elem_edge.RowSize(elem_id); const int *edges = elem_edge.GetRow(elem_id); for (int edgei = 0; edgei \u0026lt; num_edges; edgei ++) { int edge_id = edges[edgei]; .... Do something with the edge ID .... } } 上面表格中并没有提供edge2face连接关系，但可以利用Transpose方法得到：\n1 2 3 4 5 6 7 8 Table \u0026amp;face_edge = *mesh.GetFaceEdgeTable(); Table edge_face; Transpose(face_edge, edge_face); int num_edges = mesh.GetNEdges(); for (int edge_id = 0; ei \u0026lt; num_edges; edge_id++) { .... } Local index and global index\n功能 代码 局部单元数 Mesh::GetNE() 全局单元数 ParMesh::GetGlobalNE() 全局编号 局部编号+该进程的offset 给定局部编号，返回全局编号 ParMesh::GetGlobalElementNum(local_element_num) 给定全局单元编号，返回其在当前进程上局部编号 ParMesh::GetLocalElementNum(global_element_num) 获取局部单元的全局编号列表 ParMesh::GetGlobalElementIndices 单元编号有全局和局部编号，但在MFEM的Mesh类中其他网格实体只有局部编号，为了使用全局编号可用ParMesh类：\nGetGlobalVertexIndices GetGlobalEdgeIndices GetGlobalFaceIndices HypreParMatrix\nFiniteElementSpace::GetElementVDofs在vdim=1时与FiniteElementSpace::GetElementDofs效果似乎是一致的。\n计算各进程单元偏移量：\n1 2 3 4 5 6 7 8 9 10 11 int local_ne = pmesh.GetNE(); int num_procs, rank; MPI_Comm_size(pmesh.GetComm(), \u0026amp;num_procs); MPI_Comm_rank(pmesh.GetComm(), \u0026amp;rank); std::vector\u0026lt;int\u0026gt; all_ne(num_procs); MPI_Allgather(\u0026amp;local_ne, 1, MPI_INT, all_ne.data(), 1, MPI_INT, pmesh.GetComm()); std::vector\u0026lt;int\u0026gt; offsets(num_procs + 1, 0); for (int i = 0; i \u0026lt; num_procs; ++i) offsets[i+1] = offsets[i] + all_ne[i]; 计算各进程自由度偏移量：\n1 2 3 HYPRE_BigInt* dof_offsets; dof_offsets=fespace-\u0026gt;GetTrueDofOffsets(); printf(\u0026#34;rank:%d, dof:[%lld,%lld)\\n\u0026#34;, rank, dof_offsets[0], dof_offsets[1]); 其中dof_offsets[0]是当前进程的偏移量，dof_offsets[1]是下一进程的偏移量。\n获取全局自由度数目：int global_dof_num = fespace-\u0026gt;GlobalTrueVSize();\n打印当前进程tdof数目和ldof数目(包含与其他进程公共的自由度)，并转化为全局唯一tdofs：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 printf(\u0026#34;rank: %d, vdofs:%d, tdofs:%d\\n\u0026#34;, rank,fespace-\u0026gt;GetVSize(), fespace-\u0026gt;GetTrueVSize()); Array\u0026lt;int\u0026gt; ldofs,global_index_of_ldofs,global_index_of_tdofs; ldofs.SetSize(fespace-\u0026gt;GetVSize()); fespace-\u0026gt;GetVDofs(0, ldofs); global_index_of_ldofs.SetSize(fespace-\u0026gt;GetVSize()); for (int i=0; i\u0026lt;ldofs.Size(); i++){ global_index_of_ldofs[i] = fespace-\u0026gt;GetGlobalTDofNumber(ldofs[i]); } printf(\u0026#34;rank: %d, ldofs size: %d\\n\u0026#34;, rank, global_index_of_ldofs.Size());global_index_of_ldofs.Print(); global_index_of_tdofs.SetSize(fespace-\u0026gt;GetTrueVSize()); for (int i=0; i\u0026lt;global_index_of_tdofs.Size(); i++){ global_index_of_tdofs[i] = fespace-\u0026gt;GetGlobalTDofNumber(i); } printf(\u0026#34;rank: %d, tdofs size: %d\\n\u0026#34;, rank, global_index_of_tdofs.Size());global_index_of_tdofs.Print(); 返回结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 rank: 1, vdofs:6, tdofs:3 rank: 1, vdofs size: 6 0 6 2 3 7 8 rank: 1, tdofs size: 3 0 6 2 rank: 0, vdofs:6, tdofs:6 rank: 0, vdofs size: 6 0 1 2 3 4 5 rank: 0, tdofs size: 6 0 1 2 3 4 5 ","date":"2025-06-14T08:38:56+08:00","permalink":"https://baichuan-blog.netlify.app/p/mfem/","title":"MFEM"},{"content":"云的运动方程 大气动力——热力学方程组（忽略分子粘性）中的运动方程： \\[\\frac{d\\cuti{V}}{dt}=-\\frac1{\\rho}\\nabla p+\\cuti{g}-2\\cuti{\\Omega}\\times \\cuti{V}\\tag{1}\\] 其中$\\cuti{g} = (0,0,-g)^T$。\n积云对流有如下特点：\n尺度小，地球旋转产生的科氏力可略去不计。 非静力平衡，垂直运动加速度$\\f{d w}{dt}$不能被忽略。 积云对流具有高度的湍流性，需要考虑湍流粘性力的作用。 结合上述几点，当云中含液态水时，运动方程(1)应改为： \\[\\frac{d\\cuti{V}}{dt}=-\\frac1{\\rho}\\nabla p+\\cuti{g}+q_w\\cuti{g}+D_{\\cuti{V}}\\] 其中$\\rho=\\rho_d+\\rho_v$是湿空气密度，$q_w$为液态水比含水量，$\\rho(1+q_w)=\\rho_d+\\rho_v+\\rho_w=\\rho_d+\\rho_v+\\rho_c+\\rho_r$是湿空气与液体水物质的总密度（若有固态水，也以同样方法处理），$D_{\\cuti{V}}=K_m\\nabla^2\\cuti{V}$为湍流粘性力。\n方程两边同时乘以$\\rho$得到： \\[\\rho\\frac{d\\cuti{V}}{dt}=-\\nabla p+\\rho\\cuti{g}+q_w\\cuti{g}+\\rho D_{\\cuti{V}}\\tag{2}\\] 将$p,\\rho,T$分裂为背景场和扰动场： $$\\begin{cases}p = p_0(z)+p',\\\\\\rho=\\rho_0(z)+\\rho',\\\\T = T_0(z)+T'\\end{cases}\\tag{3}$$ 满足静力平衡条件： \\[\\f{\\p p_0}{\\p z}=-\\rho_0g\\] 对于积云对流，扰动温度$T'$很少超过$10K$，扰动压力$p'$很少超过$10$hPa（$p_{00}=1000$hPa），故： \\[\\begin{cases}\\f{p'}{p_0}\\ll 1\\\\\\f{\\rho'}{\\rho_0}\\ll 1\\\\\\f{T'}{T_0}\\ll 1\\end{cases}\\] 将(3)代入(2)得到： \\[\\rho\\frac{d\\cuti{V}}{dt}=-\\nabla p'+\\rho'\\cuti{g}+\\rho q_w\\cuti{g}+\\rho D_{\\cuti{V}}\\tag{4}\\]湿空气的状态方程为： \\[\\begin{cases}\\theta = T(\\f{p_{00}}{p})^{\\f{R}{c_p}}\\\\ p = \\rho R T =\\rho R_d T(1+0.608q_v)\\end{cases}\\] 其中$R=(1-q_v)R_d+q_vR_v=R_d(1-q_v+q_v\\f{R_d}{R_v})\\approx R_d(1+0.608 q_v)$。\n对$p = \\rho RT$和$p_0 = \\rho_0 RT$两边同时取对数并相减得到： \\[\\ln \\f{p_0+p'}{p_0}=\\ln\\f{\\rho_0+\\rho'}{\\rho_0}+\\ln \\f{T_0+T'}{T_0}+\\ln\\f{1+0.608((q_v)_0+(q_v)')}{1+0.608(q_v)_0}\\] 于是有一阶近似： \\[\\f{p'}{p_0}\\approx \\f{\\rho'}{\\rho_0}+\\f{T'}{T_0}+0.608\\f{(q_v)'}{1+0.608(q_v)_0}\\] 由于比湿$(q_v)_0\\ll 1$，因此对上式可做近似如下： \\[\\f{p'}{p_0}\\approx \\f{\\rho'}{\\rho_0}+\\f{T'}{T_0}+0.608(q_v)'\\tag{5}\\]当数值方法中无法直接求得密度时，我们可利用(5)将$\\rho'$代入(4)中得到： \\[\\rho\\frac{d\\cuti{V}}{dt}=-\\nabla p'+\\rho_0(\\f{p'}{p_0}-\\f{T'}{T_0}-0.608(q_v)')\\cuti{g}+\\rho q_w\\cuti{g}+\\rho D_{\\cuti{V}}\\] 此时将其余$\\rho$用平衡解$\\rho_0$代替，可得： \\[\\frac{d\\cuti{V}}{dt}=-\\f{1}{\\rho_0}\\nabla p'+(\\f{p'}{p_0}-\\f{T'}{T_0}-0.608(q_v)')\\cuti{g}+q_w\\cuti{g} + D_{\\cuti{V}}\\quad\\tag{6}\\]不过，由于我们的数值方法可以直接计算出$\\rho$，因此直接采用方程(4)即可： \\[\\rho\\frac{d\\cuti{V}}{dt}=-\\nabla p'+\\rho'\\cuti{g}+\\rho (q_c+q_r)\\cuti{g}+\\rho D_{\\cuti{V}}\\]参考文献 大气物理学. (2023). 盛裴轩等. 13.1节 ","date":"2025-06-09T10:17:10+08:00","permalink":"https://baichuan-blog.netlify.app/p/%E7%A7%AF%E4%BA%91%E5%8A%A8%E5%8A%9B%E5%AD%A6/","title":"积云动力学"},{"content":"Linux常用命令 快捷键：\n功能 Linux PowerShell 删除光标到行首所有内容 Ctrl+U Esc 删除光标前一个词 Ctrl+W Ctrl+Backspace 删除光标后一个词 Alt+D - 光标移动到行首 Ctrl+A/Home Home 光标移动到行尾 Ctrl+E/End End 切换回上一次cd进入的目录 cd - 用pushd/popd实现目录的进出栈切换 查找某个文件：\n1 find /path/to/folder -name \u0026#34;*name*\u0026#34; 其中*是通配符，功能是找出/path/to/folder及其所有子目录中查找文件名包含“name”的文件。\n查找包含某内容的文件：\n1 grep -rl \u0026#34;something-to-find\u0026#34; /path/to/folder 其中-r表示递归查找子目录，-l表示只输出包含匹配内容的文件名。\n压缩与解压缩：\n1 2 3 4 5 6 7 8 # 打包并压缩为 tar.gz tar -czvf [*.tar.gz] /path/to/folder # 解压tar.gz文件 tar -xzvf [*.tar.gz] # 递归压缩为zip zip -r [*.zip] /path/to/folder # 解压zip文件到指定文件夹 unzip [*.zip] -d [output_dir] 其中 tar 命令参数含义如下：\nc：create，创建归档文件（archive） z：gzip，使用 gzip 压缩 v：verbose，显示详细过程 f：file，指定归档文件名 x：extract，解包（解压缩） ","date":"2025-06-08T16:47:11+08:00","permalink":"https://baichuan-blog.netlify.app/p/linux/","title":"Linux"},{"content":"集群上使用fealpy 安装 安装miniconda：\n1 2 3 4 mkdir -p ~/miniconda3 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 rm ~/miniconda3/miniconda.sh 激活conda（每次进入集群都需要重新激活）：\n1 source ~/miniconda3/bin/activate 新建conda环境并激活该环境：\n1 2 conda create -n fealpy python=3.12 conda activate fealpy 安装高版本的现代编译工具链（否则pip install将报错）：\n1 2 3 4 conda install -c conda-forge gcc=12 gxx=12 gfortran=12 libgcc -y # 验证gcc, g++版本 gcc -v g++ -v 安装fealpy：\n1 pip install fealpy 会将fealpy及其需要的包（如numpy，scipy,vtk等也一同安装），新安装的包在路径~/miniconda3/envs/fealpy/lib/python3.12/site-packages下，因此也可直接将fealpy等程序包直接复制到该文件夹中。\n使用 运行脚本\n","date":"2025-06-08T12:41:56+08:00","permalink":"https://baichuan-blog.netlify.app/p/fealpy/","title":"FEALPy"},{"content":"定常问题 \\[\\a u+\\vec{\\b}\\cdot \\nabla u=f\\]光滑解 $\\mathrm{div}\\vec{\\b}=0$, 齐次边值\n$\\Omega=[0,1]^2$, $\\a=1$, $\\vec{\\beta} = (1,1)^T$, $f = u+(2x-1)y(y-1)+x(x-1)(2y-1)$, \\[u=x(x-1)y(y-1).\\] $\\mathrm{div}\\vec{\\b}=0$, 非齐次边值\n$\\Omega=[0,1]^2$, $\\a=1$, $\\vec{\\beta} = (1,1)^T$, $f = u+\\pi \\sin(\\pi (x+y))$, \\[u=\\sin(\\pi x)\\sin(\\pi y)+1.\\] $\\mathrm{div}\\vec{\\b}\\ne 0$\n$\\Omega=[0,1]^2$, $\\a=1$, $\\vec{\\beta} = \\f12(x,y)^T$, $\\mathrm{div}\\vec{\\b}=1$, $f = u+\\f12\\pi(x \\cos(\\pi x)\\sin(\\pi y)+y\\sin(\\pi x)\\cos(\\pi y))$, \\[u=\\sin(\\pi x)\\sin(\\pi y)+1.\\] 保证$\\a-\\f12\\mathrm{div}\\vec{\\beta}\u0026gt;0$.\n间断解 $\\mathrm{div}\\vec{\\b}=0$\n$\\Omega=[0,1]^2$, $\\a=1$, $\\vec{\\b}=(-y,x)^T$, $f=0$, \\[u=C(r)e^{-\\theta},\\quad C(r)=\\begin{cases}1,\\ r\\in(0.2,0.4)\\\\0,\\ \\text{otherwise}\\end{cases},\\] 其中$r=\\sqrt{x^2+y^2}$, $\\theta=\\mathrm{arctan}(\\frac{y}{x})$.\n构造思路: 取与$(x,y)$相容的另一组坐标对$(\\theta,r)$, 使得$\\vec{\\b}=(\\p_{\\theta}x,\\p_{\\theta}y)^T$, 记$u=u(x,y)=\\tilde{u}(\\theta,r)$则 $$\\begin{aligned} \u00260=\\a u+\\b\\cdot \\nabla u=\\a u+\\p_{\\theta} \\tilde{u}=e^{-\\a\\theta}\\p_{\\theta}(e^{\\a\\theta}u),\\\\ \\implies \u0026 u=C(r)e^{-\\a \\theta}.\\end{aligned}$$ $\\mathrm{div}\\vec{\\b}\\ne 0$\n$\\Omega=[\\f12,1]^2$, $\\a=2$, $\\vec{\\beta}=(1,\\frac{y}{x})^T$, $\\mathrm{div}\\vec{\\b}=\\f1x$, $f=0$, \\[u = C(\\f{y}{x})e^{-\\a \\theta},\\quad C(r)=\\begin{cases}1,\\ \\f{y}{x}\\in(\\f12,1)\\\\0,\\ \\f{y}{x}\\in(1,2)\\end{cases},\\] 选择参数对$(\\theta=x,r=\\frac{y}{x})$, 此时\n$$\\f{\\p (\\theta,r)}{\\p (x,y)}=\\begin{vmatrix}1\u00260 \\\\ -\\f{y}{x^2}\u0026\\f1{x}\\end{vmatrix}\\ne0.$$ 非定常问题 \\[\\p_t u+\\vec{\\beta}\\cdot\\nabla u = 0,\\quad u(\\mathbf{x},0)=u_0(\\mathbf{x})\\] 下面两个算例来自Guermond二阶保正格式.\n光滑解 $\\Omega=[-1,1]^2$, $\\vec{\\b}=2\\pi(-y,x)^T$, \\[u_0(\\mathbf{x})=\\f12(1-\\tanh(\\f{(\\mathbf{x}-\\mathbf{x}_0)^2}{r_0^2}-1)),\\quad r_0=0.25, \\mathbf{x}_0=(0.3,0)\\] 其中$\\tanh x=\\f{e^{x}-e^{-x}}{e^{x}+e^{-x}}$. 用特征线法计算精确解, \\[\\begin{cases}x(t)=x(0)\\cos(2\\pi t)-y(0)\\sin(2\\pi t)\\\\ y(t) = x(0)\\sin(2\\pi t)+y(0)\\cos(2\\pi t)\\end{cases},\\] \\[\\begin{pmatrix}x(0)\\\\y(0)\\end{pmatrix}=\\begin{pmatrix}x\\cos(2\\pi t)+y\\sin(2\\pi t)\\\\ -x\\sin(2\\pi t)+y\\cos(2\\pi t)\\end{pmatrix},\\] 于是 \\[u(t,x,y)=u_0(x\\cos(2\\pi t)+y\\sin(2\\pi t),-x\\sin(2\\pi t)+y\\cos(2\\pi t)).\\] 当$t=1$时, $u(1,x,y)=u_0(x,y)$, 解图像见图1.\n图2: smooth solution 图3: three body rotation 间断解: Three body rotation 将上面光滑解算例的初值改为: $$ u_0(\\mathbf{x}) = \\begin{cases} 1 \u0026 \\text{if $\\|\\mathbf{x} - \\mathbf{x}_d\\| \\leq r_0$ and $(|x| \\geq 0.05$ or $y \\geq 0.7$),} \\\\ 1 - \\frac{\\|\\mathbf{x} - \\mathbf{x}_c\\|}{r_0} \u0026 \\text{if $\\|\\mathbf{x} - \\mathbf{x}_c\\| \\leq r_0$}, \\\\ g(\\|\\mathbf{x} - \\mathbf{x}_h\\|) \u0026 \\text{if $\\|\\mathbf{x} - \\mathbf{x}_h\\| \\leq r_0$}, \\\\ 0 \u0026 \\text{otherwise}, \\end{cases} $$ 其中$r_0=0.3$, $g(r)=\\f14(1+\\cos(\\pi\\min(\\f{r}{r_0},1)))$, $\\mathbf{x}_d=(0,0.5)$, $\\mathbf{x}_c=(0,-0.5)$, $\\mathbf{x}_h=(-0.5,0)$. 解图像见图3.\n","date":"2025-05-30T21:12:27+08:00","permalink":"https://baichuan-blog.netlify.app/p/%E8%BE%93%E8%BF%90%E6%96%B9%E7%A8%8B%E7%AE%97%E4%BE%8B/","title":"输运方程算例"},{"content":"本文总结了Git的常用操作：第1-5节是基础操作，包括仓库初始化、添加身份标识等，第5节的暂存-提交-推送-拉取是日常管理中使用最为频繁的命令；后面几节则是一些进阶操作，包括解决合并冲突、临时保存、处理提交历史等，以及SSH免密连接远程仓库以及不同机器间的远程传输。\n初始化及绑定远程仓库 本地从零开始构建仓库：\n1 2 3 4 # 初始化仓库 git init # 绑定远程仓库 git remote add origin [url] 其中origin是默认的远程仓库名字， [url]是远程仓库地址。 远程仓库的默认名可通过下面语句修改：\n1 git remote rename origin [newname] 另一种初始化方法是克隆现有仓库到本地：\n1 git clone [url] [name] 其中[name]是本地仓库名称（地址）。\n查看绑定的远程仓库：\n1 git remote -v 如果需要更改绑定的远程仓库：\n1 git remote set-url origin [newurl] 添加身份标识 Git每次commit会记录提交者信息，因此我们需要先配置个人信息：\n1 2 git config user.name \u0026#34;\u0026#34; git config user.email \u0026#34;\u0026#34; 默认只为当前仓库添加身份标识，若添加--global选项则可为所有仓库添加统一的个人信息。git config --list --local可以查看当前仓库配置信息。\n分支管理 1 git branch -m [old_branch] [new branch] -m参数不覆盖已有同名分支，-M参数强制重命名，将覆盖已有的同名分支。 如果是对当前分支进行重命名，可省略[old_branch]。\n在git init初始化后，初始分支名为master，建议重命名为main：\n1 git branch -m main 更多操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 查看本地分支 git branch # 查看远程分支 git branch -r # 新建本地分支 git branch [new_branch] # 删除本地分支 git branch -d [branch] # 新建分支并切换到该分支 git checkout -b [new_branch] # 切换到已有分支 git checkout [branch] # 删除远程分支 git push origin --delete [branch] 查看工作区和暂存区状态 1 git status 后面可以加-s（--short的缩写），用以输出更简洁、紧凑的内容，此时若显示\n1 2 3 M file1 A file2 ?? file3 表示file1被修改（modified），file2被新添加（added），file3未跟踪。\n暂存-提交-推送-拉取 这是最常用的操作：\n1 2 3 4 5 6 7 8 9 10 # 拉取远程仓库origin/main合并到当前分支 git pull origin main # 将文件添加到暂存区 git add file # 删除已暂存的所有文件 git rm -r --cached . # 提交并注释 git commit -m \u0026#34;message\u0026#34; # 将本地分支main推送到远程origin/main git push -u origin main 其中-u表示origin是main的上游。git pull origin main和git push -u origin main只需设置一次，之后可以简单使用git pull和git push，Git会默认使用此前仓库的关联方式。\n常用git add .把所有新文件及变动过的文件一次性添加到暂存区，此时为了过滤掉某些文件，可在仓库根目录新建.gitignore文件，在里面写入需要过滤的文件。\n解决合并冲突 假设现在有两个分支：main和another，将another的修改内容合并到main：\n1 2 3 4 # 切换到主分支 git checkout main # 将another合并到main git merge another 当出现冲突时，需要打开每个冲突文件处理冲突。然后再git add告诉Git冲突已解决和git commit提交结果。\n临时保存 如果存在工作未提交，git pull或者git merge会报错，此时使用git stash允许我们先将未完成的工作先临时保存而无需commit。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 临时保存已暂存(git add)文件 git stash # 临时保存除.gitignore忽略文件以外的文件 git stash -u # 临时保存所有文件(包含.gitignore忽略的文件) git stash -a # 添加描述信息 git stash save \u0026#34;描述信息\u0026#34; # 查看stash列表 git stash list # 恢复指定的stash, 1改为相应的stash编号 git stash apply stash@{1} # 删除指定的stash, 1改为相应的stash编号 git stash drop stash@{1} # 清空stash列表 git stash clear 处理提交信息 修改提交信息 1 2 3 4 # 修改commitid之后的提交信息(不包括commitid本身) git rebase -i [commitid] # 修改commitid及其后的提交信息 git rebase -i [commitid]^ 重置分支到特定提交 1 2 3 4 5 6 # 暂存区和工作目录保持不变 git reset --soft [commitid] # 暂存区重置, 工作目录不变 git reset --mixed [commitid] # 完全重置到指定提交 git reset --hard [commitid] git reset [commitid]默认采用--mixed，所谓暂存区重置是恢复快照（每次commit提交时Git都会记录下这一刻所有文件的样子），实际上从vscode的源代码管理中看暂存区是空的，工作区则仍是修改后的文件，不会被reset覆盖。该操作可用于合并提交历史。\n远程提交 1 git pull --rebase origin main 从远程仓库origin拉取分支main的最新提交，然后将本地分支的提交挪到远程的最新提交之后，这可以保持提交历史的线性，适合于个人开发或是希望获得简洁历史的协作场景。\n可以通过下面配置默置使git pull默认采用--rebase模式：\n1 git config --global pull.rebase true 如果仅希望对当前仓库生效，则去掉--global选项。\nSSH免密连接 生成密钥对：\n1 ssh-keygen -m PEM -t ed25519 -C \u0026#34;tag\u0026#34; -f ~/.ssh/key 随后按Enter键直到产生密钥。其中PEM为密钥格式，ed25519为生成密钥的算法，\u0026quot;tag\u0026quot;为密钥添加的注释、标识，~/.ssh/key为密钥的保存路径。\n查看生成的公钥并复制粘贴到远程平台：\n1 cat ~/.ssh/key.pub 设置SSH配置文件以指定连接远程平台所使用的用户名和私钥。以Github为例，在~/.ssh/config中\n1 2 3 4 5 Host github.com # 别名 HostName github.com # 远程主机名 IdentityFile ~/.ssh/github IdentitiesOnly yes # 只使用指定的IdentityFile ServerAliveInterval 60 # 保持连接，每60秒发送一次心跳 测试github是否可以连接可以使用：\n1 ssh -T git@github.com 如果是连接集群的话，还需要设置字段：\n1 2 User [user] # 集群上的个人用户名 Port [port] # 端口号 另外还需将Hostname改为要访问的ip地址，如果设置Host lssc，则每次ssh连接时只需输入命令：\n1 ssh lssc 等价于：\n1 ssh [user@]hostname -p [port] 远程传输文件 1 scp [选项] [源文件] [目标路径] 选项-r递归复制（用于复制文件夹），-P指定远程端口。\n以本地与集群lssc间文件传输为例：\n1 2 scp lssc:/home/user/file ./file scp ./file lssc:/home/user/file 参考资料 Git教程 ","date":"2025-05-27T18:46:25+08:00","permalink":"https://baichuan-blog.netlify.app/p/git/","title":"Git"},{"content":"* 的渲染 Markdown行内公式$a^*b^*$渲染会出错（两个*间会变成斜体$a^b^$），改成$a^\\*b^\\*$（$a^*b^*$），但这样在vscode中的渲染有问题，一种安全的写法是$a^\\ast b^\\ast$（$a^\\ast b^\\ast$）。\n行内公式渲染问题 有时候行内公式用$$渲染不出，可以改用\\(\\)的方式。\n引用自己的博客文章 [](../article-name)，其中article-name是要引用的文章名（以url中的文章名为主）。例如，引用文章Markdown Syntax Guide时为../markdown-syntax-guide，效果：Markdown Syntax Guide。\n自定义数学命令 全局定义 所有开启math=true的文章中均可使用。\n在layouts\\partials\\article\\components\\math.html中的ignoredClasses: [\u0026quot;gist\u0026quot;]前面添加：\n1 2 3 4 macros: { \u0026#34;\\\\abs\u0026#34;: \u0026#34;\\\\left| #1 \\\\right|\u0026#34;, \u0026#34;\\\\Sob\u0026#34;: \u0026#34;W^{#1,#2}(#3)\u0026#34; // 三参数：k, p, 区域 }, 效果为：\n$\\abs{x + y}$：$\\abs{x + y}$ $\\Sob{1}{2}{\\Omega}$：$\\Sob{1}{2}{\\Omega}$ $$ \\gdef\\wkp#1#2#3{W^{#1,#2}(#3)} $$局部定义 仅限于定义所在的文章从定义后到文章结束的部分。\n$$\\gdef\\wkp#1#2#3{W^{#1,#2}(#3)}$$：$\\wkp{1}{2}{\\Omega}$\n使用标题但不编号 \u0026lt;h5\u0026gt; 标题 \u0026lt;/h5\u0026gt;可以产生五级标题效果，但不会参与目录的编号。\n标题 主页菜单添加新网站 hugo.yaml：\n1 2 3 4 5 6 7 8 menu: social: - identifier: bilibili name: bilibili weight: 2 url: https://www.bilibili.com/ params: icon: bilibili 将网站icon添加到assets\\icons中，可到iconfont下载。\n","date":"2025-05-25T11:07:01+08:00","permalink":"https://baichuan-blog.netlify.app/p/hugo-markdown%E6%8A%80%E5%B7%A7/","title":"Hugo Markdown技巧"},{"content":"微物理过程参数化 本文介绍积云动力学中的水分平衡方程、位温方程以及相应的微物理参数化方法. 云内的水分包含水汽、液态水和固态水[1, Sec.13.1 p353], 本文不考虑冰相(忽略固态水), 将水汽(water vapor)、云水(cloud water)、雨水(rain water)的密度分别记为$\\rho_v,\\rho_c,\\rho_r$, 干空气密度为$\\rho$, 记水汽混合比、云水混合比、雨水混合比分别为$q_v=\\frac{\\rho_v}{\\rho},q_c=\\frac{\\rho_c}{\\rho},q_r=\\frac{\\rho_r}{\\rho}$.\n水汽、云水、雨水之间可相互转化, 记为:\n$P_1$: 水汽$\\to$云水, 水汽凝结速率; $P_2$: 云水$\\to$雨水, 包含云水自动转化速率(rain auto-conversion)$A_r$和碰并增长速率(rain accretion)$C_r$; $P_6$: 云水$\\to$水汽, 云滴蒸发速率; $P_7$: 雨水$\\to$水汽, 雨滴蒸发速率. 水汽、云水、雨水方程分别为: $$\\begin{aligned} \\frac{dq_v}{dt} \u0026 = D_{q_v}+P_6 -P_1 + P_7 \\\\ \\frac{dq_c}{dt} \u0026 = D_{q_c}-(P_6-P_1) - P_2 \\\\ \\frac{dq_r}{dt} \u0026 = D_{q_r}+\\frac{1}{\\rho}\\frac{\\partial}{\\partial z}(\\rho v_rq_r)+P_2 - P_7 \\end{aligned}$$ 水汽凝结释放潜热, 使位温升高; 云滴、雨滴蒸发吸收潜热, 使位温降低, 由此得到位温方程 $$\\frac{d\\theta}{dt} = D_\\theta-\\gamma(P_6-P_1+P_7)$$ $\\gamma =\\frac{L_v}{c_p\\Pi}$, 其中$L_v$是蒸发潜热, $c_p$是定压比热容, $\\Pi=(\\frac{p}{p_0})^{\\frac R{c_p}}$是Exner pressure(压力无量纲化).\n上面$P_i$是[1]中记号, 我们将云水转化为水汽的$P_6-P_1$合并, 其实恰好是饱和水汽混合比$q_{vs}$(即单位质量干空气所能容纳的最大饱和水汽质量)的转化速率, 即$P_6-P_7=\\frac{d q_{vs}}{dt}$, 再将$P_7$改用$E_r$(rate of rain evaporation), 则得到[2]中方程组: $$\\begin{aligned} \\frac{d\\theta}{dt} \u0026 = D_\\theta-\\gamma(\\frac{d q_{vs}}{dt}+E_r)\\\\ \\frac{dq_v}{dt} \u0026 = D_{q_v}+ \\frac{d q_{vs}}{dt}+E_r\\\\ \\frac{dq_c}{dt} \u0026 = D_{q_c}- \\frac{d q_{vs}}{dt} - A_r-C_r \\\\ \\frac{dq_r}{dt} \u0026 = D_{q_r}+\\frac{1}{\\rho}\\frac{\\partial}{\\partial z}(\\rho v_rq_r)+A_r+C_r -E_r \\end{aligned}\\tag{1} $$观察可发现: $$\\begin{aligned} \u0026\\frac{d \\theta}{dt}+\\gamma\\frac{d q_{v}}{dt} = D_{\\theta}+\\gamma D_{q_v}\\\\ \u0026\\frac{d q_v}{dt}+\\frac{d q_c}{dt} = D_{q_v}+D_{q_c} \\end{aligned}\\tag{2}$$ 事实上, 这由液态、气态水间转化容易看出.\n公式($1$)和动量方程、连续性方程组以及状态结合可得到完整大气系统. 我们采用两步方法求解该系统: 首先忽略水汽、云水、雨水间的转化(1)右端只保留$D_{(\\cdot)}$项, 将解推进一个时间步, 得到$\\rho^{n+1},\\vec{u}^{n+1}$, $\\theta^{\\ast},q_{v}^\\ast,q_c^\\ast,q_r^\\ast$; 随后利用转化速率项对$\\theta^{\\ast},q_{v}^\\ast,q_c^\\ast,q_r^\\ast$做调整得到$\\theta^{n+1},q_v^{n+1},q_c^{n+1},q_r^{n+1}$. 相当于在原来不考虑水汽情形求解完一个时间步后对解变量做后处理.\n下面推导微物理转化项的参数化方法, 由[2]有: $$\\begin{aligned} \\text{rain auto-conversion:}\\ \u0026A_r = k_1(q_c-a),\\quad k_1=10^{-3}s^{-1}, a = 10^{-3}g\\ g^{-1}\\\\ \\text{rain accretion:}\\ \u0026C_r = k_2q_cq_r^{0.875},\\quad k_2 = 2.2s^{-1}\\\\ \\text{rain evaporation:}\\ \u0026E_r = \\frac{1}{{ \\rho}} \\frac{(1 - q_v / q_{v s}) (1.6 + 124.9 ( \\rho q_r)^{0.2046})({\\rho} q_r)^{0.525}}{5.4 \\times 10^5 + 2.55 \\times 10^6 / ( {p}q_{v s})}\\\\ \\text{saturation water vapor fraction:}\\ \u0026q_{vs} = \\frac{3.8}{ {p}}\\exp(17.27\\frac{ \\Pi\\theta-273}{ \\Pi\\theta-36}),\\quad(\\text{Teten's formula})\\\\ \\text{terminal velocity of rain following:}\\ \u0026v_r=3634 (\\rho q_r)^{0.1346}(\\frac{\\rho}{\\rho_0})^{-\\frac 12}\\ [\\text{cm\\ s}^{-1}] \\end{aligned}$$ 其中$\\rho_0$是地面的基态密度. 注意此处的单位, $p$是百帕mb(millibar), $\\rho$是$g/cm^3=10^{3}kg/m^3$, $q_v,q_c,q_r$均为$g\\ g^{-1}$.\n$A_r,C_r,\\frac{1}{\\rho}\\frac{\\partial}{\\partial z}(\\rho v_r q_r)$\n$$\\begin{aligned} \u0026\\theta^{(1)}=\\theta^*\\\\ \u0026q_v^{(1)}=q_v^*\\\\ \u0026q_c^{(1)}=q_c^*-\\Delta t (A_r+C_r)\\\\ \u0026q_r^{(1)}=q_r^*+\\Delta t(\\frac{1}{\\rho}\\frac{\\partial}{\\partial z}(\\rho v_rq_r^*)+A_r+C_r ) \\end{aligned}$$ 其中$\\frac{1}{\\rho}\\frac{\\partial}{\\partial z}(\\rho v_rq_r^\\ast)$的CG或DG处理参考[5].\n$\\frac{d q_{vs}}{dt}$\n将(1)中$\\theta, q_v, q_c$中已经处理好的右端项去掉, 类似(2)得到 $$\\begin{aligned} \u0026\\frac{\\partial\\theta}{\\partial t}+\\gamma\\frac{\\partial q_v}{\\partial t}=0\\\\ \u0026\\frac{\\partial q_v}{\\partial t}+\\frac{\\partial q_c}{\\partial t}=0 \\end{aligned}$$ 于是 $$\\theta^{(2)}+\\gamma q_v^{(2)}=\\theta^{(1)}+\\gamma q_v^{(1)} \\tag{3}$$ $$q_v^{(2)}+q_c^{(2)} = q_v^{(1)}+q_c^{(1)}\\tag{4}$$利用Teten\u0026rsquo;s formula, 将$q_v$在$\\theta^*$处做一阶Taylor展开有: $$q_v^{(2)}=q_{vs}^{(2)}\\approx q_v^{(1)}(1+\\frac{4093\\Pi}{(\\Pi\\theta^{(1)}-36)^2}(\\theta^{(2)}-\\theta^{(1)}))\\tag{5}$$第一个等号是由饱和假设, 假如$q_v^{(1)} \u0026gt;q_{vs}^{(1)}$, 此时过饱和, 将会通过转换到达饱和状态[3, Appendix: Saturation Technique]. 将该式代入(3)可得: $$\\theta^{(2)} = \\theta^{(1)}+\\frac{\\gamma}{1+\\gamma q_{vs}^{(1)}\\frac{4039\\Pi}{(\\Pi\\theta^{(1)}-36)^2}}(q_v^{(1)}-q_{vs}^{(1)})$$ 将上式再代入(3)可得: $$q_v^{(2)} = q_v^{(1)}- \\frac{1}{1+\\gamma q_{vs}^{(1)}\\frac{4039\\Pi}{(\\Pi\\theta^*-36)^2}}(q_v^{(1)}-q_{vs}^{(1)})$$ 将上式代入(4)可得: $$q_c^{(2)} = q_c^{(1)}+q_v^{(1)}-q_v^{(2)}$$ 为了保证$q_c\\ge 0$, 令 $$\\begin{aligned} q_c^{(2)} \u0026= \\max(q_c^{(1)}+q_v^{(1)}-q_v^{(2)},0)\\\\ \u0026 = q_c^{(1)}+\\max(\\text{prod},-q_c^{(1)})\\\\ (\\text{prod} :\u0026=q_v^{(1)}-q_v^{(2)}=\\frac{1}{1+\\gamma q_{vs}^{(1)}\\frac{4039\\Pi}{(\\Pi\\theta^{(1)}-36)^2}}(q_v^{(1)}-q_{vs}^{(1)}) )\\end{aligned}$$ 再次利用(4)和(3)得到: $$\\begin{aligned} q_v^{(2)}\u0026=\\max(q_v^{(1)}+q_c^{(1)}-q_c^{(2)},0)\\\\ \u0026 = \\max(q_v^{(1)}-\\max(\\text{prod},-q_c^{(1)}),0)\\\\ \\theta^{(2)}\u0026 = \\theta^{(1)}+\\gamma \\max(\\text{prod},-q_c^{(1)}) \\end{aligned}$$ $E_r$\n结合$q_v,q_r$非负, 得到: $$\\begin{aligned} \u0026\\theta^{(n+1)} = \\theta^{(2)}-\\Delta t \\gamma E_r\\\\ \u0026q_v^{(n+1)}=\\max(q_v^{(2)}+\\Delta t E_r,0)\\\\ \u0026q_r^{(n+1)}=\\max(q_r^{(1)}-\\Delta t E_r,0) \\end{aligned}$$ 综上, 可将算法重新总结如下: $$ \\begin{aligned} \u0026(\\vec{A}_r)_i = \\langle k_1(q_c-a),\\varphi_i\\rangle \\\\ \u0026(\\vec{C}_r)_i = \\langle k_2q_cq_r^{0.875},\\varphi_i \\rangle \\\\ \u0026\\Pi = (\\frac{p}{p_0})^{\\frac{R}{c_p}},\\quad q_{vs} = \\frac{3.8}{ {p}}\\exp(17.27\\frac{ \\Pi\\theta-273}{ \\Pi\\theta-36}),\\quad \\gamma = \\frac{L_v}{c_p\\Pi}\\\\ \u0026v_r=3634 (\\rho q_r)^{0.1346}(\\frac{\\rho}{\\rho_0})^{-\\frac 12}\\\\ \u0026(\\overset\\longrightarrow{\\text{sed}})_i=\\sum_K\\frac{1}{\\rho_K}(\\int_{\\p K}\\rho v_r q_r\\varphi_i \\vec{n}_z\\d \\sigma -\\int_{K} \\rho v_r q_r\\p_z\\varphi_i\\d x )\\\\ \u0026(\\overset\\longrightarrow{\\text{prod}})_i=\\inn{\\frac{1}{1+\\gamma q_{vs}\\frac{4039\\Pi}{(\\Pi\\theta 36)^2}}(q_v-q_{vs})}{\\varphi_i}\\\\ \u0026(\\vec{E}_r)_i = \\inn{\\frac{1}{{ \\rho}}\\max(1 - \\f{q_v}{q_{vs}},0) \\frac{(1.6 + 124.9 ( \\rho q_r)^{0.2046})({\\rho} q_r)^{0.525}}{5.4 \\times 10^5 + 2.55 \\times 10^6 / ( {p}q_{v s})}}{\\varphi_i}\\\\ \\\\ \u0026q_c^{(1)}=q_c-\\Delta t M^{-1}(\\vec{A}_r+\\vec{C}_r)\\\\ \u0026\\theta^{n+1} =\\theta+\\gamma (\\max(M^{-1}\\overset\\longrightarrow{\\text{prod}},-q_c^{(1)})-\\D t M^{-1}\\vec{E}_r) \\\\ \u0026 q_v^{n+1} = \\max(q_v-\\max(M^{-1}\\overset\\longrightarrow{\\text{prod}},-q_c^{(1)})+\\D t M^{-1}\\vec{E}_r,0)\\\\ \u0026q_c^{n+1}=q_c^{(1)}+\\max(M^{-1}\\overset\\longrightarrow{\\text{prod}},-q_c^{(1)})\\\\ \u0026q_r^{n+1}=\\max(q_r+\\Delta t M^{-1}(\\vec{A}_r+\\vec{C}_r+\\overset\\longrightarrow{\\text{sed}}-\\vec{E}_r),0 ) \\end{aligned} $$ 其中$M$为质量矩阵, $\\varphi_i$为测试函数.\n注：在实际代码中，由于$\\rho,p$是$P_2$元, 为方便先将两者投影到$DG_1$空间.\n参考文献 大气物理学. (2023). 盛裴轩等\nThe simulation of three-dimensional convective storm dynamics. (1978). Klemp, J., Wilhelmson, R.\nA comparison between axisymmetric and slab-symmetric cumulus cloud models. (1973). Soong, S., Ogura, Y.\nIdealized global nonhydrostatic atmospheric test cases on a reduced-radius sphere. (2015). J. B. Klemp, W. C. Skamarock, S.-H. Park\nA Non-Column Based, Fully Unstructured Implementation of Kessler\u0026rsquo;s Microphysics With Warm Rain Using Continuous and Discontinuous Spectral Elements. (2023). Yassine Tissaoui, Simone Marras, etc.\n","date":"2025-05-25T09:47:13+08:00","permalink":"https://baichuan-blog.netlify.app/p/kessler-microphysical-parameterization/","title":"Kessler Microphysical Parameterization"},{"content":"Hugo搭建个人博客\nStack主题参数设置 Hugo会优先从主路径文件夹读取文件，若没找到，则进入hugo.yaml设置的theme中寻找。因此，如果需要修改主题文件，请先将其从相应主题中复制到主路经相应位置再进行修改。\n绑定个人网站 hugo.yaml：\n1 baseurl: https://baichuan-sihai.github.io/hugo/ 主页右侧控件 hugo.yaml：\n1 2 3 4 5 6 7 8 9 10 11 12 13 params: widgets: homepage: - type: search - type: archives params: limit: 5 - type: categories params: limit: 10 - type: tag-cloud params: limit: 10 控制主页右端区域功能，limit是条目数。\n文章发布的时间格式 hugo.yaml：\n1 2 3 params: dateFormat: published: 2006-01-02 15:04 MST 其中MST为时区。\n允许数学公式 hugo.yaml：\n1 2 3 params: article: math: true 文章模板 archetypes/default.md：\n1 2 3 4 5 6 7 8 9 10 11 --- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; description: date: {{ .Date }} image: math: license: categories: tag: weight: --- categories生成分类，在文章中会显示分类；tag生成标签，不过在文章中不会显示；weight可以设置文章权重，以达到置顶功能，weight越小越靠前（主页的menu也有weight参数，可以控制菜单顺序）\n在博客主路径下，终端输入：\n1 2 3 hugo new content/post/dir/file.md # or hugo new content/post/file.md 可依据模板生成文章的front matter。根据hugo.yaml中的dateFormat, math,license为文章生成默认值。此处即使post下没有文件夹dir也会生成，可以在文件夹中放封面图片等相关内容。\n修改文章副标题字体大小 assets/scss/partials/article.scss：\n1 2 3 4 5 6 7 8 9 .article-subtitle { font-weight: normal; color: var(--card-text-color-secondary); line-height: 1.5; margin: 0; font-size: 1.5rem; // 字体大小 @include respond(xl) { font-size: 1.5rem; //大屏(屏幕宽度达到xl)时字体大小 } 添加最后更新时间 hugo.yaml：\n1 2 3 4 5 6 7 8 params: dateFormat: published: 2006-01-02 15:04 lastUpdated: 2006-01-02 15:04 enableGitInfo: true frontmatter: lastmod: [\u0026#39;lastmod\u0026#39;, \u0026#39;:git\u0026#39;, \u0026#39;date\u0026#39;, \u0026#39;publishDate\u0026#39;] 这里2006-01-02是go语言的yyyy-mm-dd，enableGitInfo: true后会自动用Git信息填充，lastmod的几个参数具体解释参见官方文档的Configure Dates。\n这时“最后更新于”显示在文末，如果想挪到前面，可将layouts\\partials\\article\\components\\footer.html的11-18行注释掉，然后将 layouts\\partials\\article\\components\\details.html的26-49行替换为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 {{ if or (not .Date.IsZero) (.Site.Params.article.readingTime) }} \u0026lt;footer class=\u0026#34;article-time\u0026#34;\u0026gt; {{ if not .Date.IsZero }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;date\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--published\u0026#34;\u0026gt; {{- .Date.Format (or .Site.Params.dateFormat.published \u0026#34;Jan 02, 2006\u0026#34;) -}} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;!-- Created Date --\u0026gt; {{- $pubdate := .PublishDate.Format \u0026#34;2006-01-02 15:04\u0026#34; }} \u0026lt;!-- Last Updated Date --\u0026gt; {{- if .Lastmod }} {{- $lastmod := .Lastmod.Format \u0026#34;2006-01-02 15:04\u0026#34; }} {{- if ne $lastmod $pubdate }} \u0026lt;span class=\u0026#34;article-time--final-update\u0026#34;\u0026gt; 最后更新于 \u0026lt;time class=\u0026#34;article-time--updated\u0026#34; datetime=\u0026#34;{{ .Lastmod }}\u0026#34; title=\u0026#34;{{ .Lastmod }}\u0026#34;\u0026gt;{{$lastmod}}\u0026lt;/time\u0026gt; \u0026lt;/span\u0026gt; {{- end }} {{- end }} {{ if .Site.Params.article.readingTime }} \u0026lt;div\u0026gt; {{ partial \u0026#34;helper/icon\u0026#34; \u0026#34;clock\u0026#34; }} \u0026lt;time class=\u0026#34;article-time--reading\u0026#34;\u0026gt; {{ T \u0026#34;article.readingTime\u0026#34; .ReadingTime }} \u0026lt;/time\u0026gt; \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/footer\u0026gt; {{ end }} 分类 archetypescategories.md是分类的模板：\n1 2 3 4 5 6 7 8 --- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; description: image: \u0026#34;images/\u0026#34; style: background: \u0026#34;\u0026#34;#ff7f7f\u0026#34;\u0026#34; color: \u0026#34;#fff\u0026#34; --- Hugo中图片路径是按其在static文件夹下的相对路径，因此可以将用作封面的图片都放在static/images/下。background为背景色，这里取的是浅红色；color是字体颜色，这里是白色。颜色可参考十六进制色表。\n定义新分类：\n1 hugo new categories/Markdown/_index.md 其中Mardown为新定义的分类。\n调节主页左侧各栏目顺序 在content/page中修改各栏目md文件的weight\nGithub Action自动化部署 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 创建博客 hugo new site myblog cd myblog # 初始化仓库 git init git config user.email \u0026#34; \u0026#34; git config user.name \u0026#34; \u0026#34; # 添加远程仓库 git remote add origin git@github.com:Baichuan-Sihai/myblog.git # 重命名本地分支 git branch -M main # 将stack主题作为子模块添加 git submodule add https://github.com/CaiJimmy/hugo-theme-stack.git themes/hugo-theme-stack # 将public文件夹作为子模块添加 git submodule add git@github.com:Baichuan-Sihai/hugo.git ./public # 暂存、添加提交信息并提交文件 git add . git commit -m \u0026#34;-\u0026#34; git push -u origin main 如果git submodule add时报错fatal: 'public' already exists in the index，则\n1 git rm -r --cached public 这只将public从索引删除，不会删除实际文件夹\ngit submodule add产生.gitmodules文件：\n1 2 3 4 5 6 [submodule \u0026#34;public\u0026#34;] path = public url = git@github.com:Baichuan-Sihai/hugo.git [submodule \u0026#34;themes/hugo-theme-stack\u0026#34;] path = themes/hugo-theme-stack url = https://github.com/CaiJimmy/hugo-theme-stack.git Netlify加速网站访问 Netlify 是一个现代化的 ​​静态网站托管与自动化部署平台​​，专为开发者设计，它提供全球 CDN、持续集成、无服务器函数等功能，让静态网站部署变得极其简单且高性能。 Netlify的免费基础套餐每月包含100GB流量、300分钟构建时长及 1 个并发构建队列，满足个人项目和小型站点需求。通过Github仓库导入Netlify中进行部署，我们可以获得更顺畅的访问体验。另外，git push会触发自动部署，我们可以一键实现推送仓库——构建页面——部署网站的一条龙服务。\n注意，需要将hugo.yaml中的baseurl改为netlify生成的网站：\n1 baseurl: https://baichuan-blog.netlify.app/ 参考资料 Stack主题 从Github仓库生成Github Pages 【雷】Hugo + Github免费搭建博客，并实现自动化部署 ","date":"2025-05-24T12:07:37+08:00","permalink":"https://baichuan-blog.netlify.app/p/hugo%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/","title":"Hugo使用技巧"},{"content":"安装windows下包管理器Scoop 最近一段时间用惯了linux的一些命令行操作后回到Windows总有点不适应，cmd中像是ls这样的命令无法使用。其实Windows下的Windows PowerShell可以使用不少像是ls,rm这样的指令，获得类似与linux类似的体验，由于Win11中Windows Terminal集成了Windows PowerSell，cmd等shell，功能强大且颜值高，故强烈推荐此后使用terminal。\n将install.ps1下载install.ps1到C:\\software\\Scoop中，然后\n1 2 3 4 5 6 7 cd C:\\software\\Scoop # 获得脚本执行权限 Set-ExecutionPolicy -Scope Process -ExecutionPolicy Bypass # 打开vpn并设置scoop代理, ip和端口号根据自己的代理设置 # scoop config proxy 127.0.0.1:7890 # 执行install.ps1 .\\install.ps1 安装Hugo 1 scoop install hugo-extended Hugo有两个版本：标准版和扩展版。使用扩展版可以：\n在处理图像时，将图像编码为WebP格式。无论您使用哪个版本，都可以解码WebP图像。 使用内置的LibSass转译器，将Sass转译为CSS。使用Dart Sass转译器时不需要扩展版。 使用hugo-theme-stack主题需要安装扩展版。\n创建新博客 1 2 cd C:\\mywork hugo new site myblog # myblog是自定义名称 完成后会有提示如下：\nCongratulations! Your new Hugo site was created in C:\\mywork\\myblog.\nJust a few more steps\u0026hellip;\nChange the current directory to C:\\mywork\\myblog. Create or install a theme: Create a new theme with the command \u0026ldquo;hugo new theme \u0026rdquo; Or, install a theme from https://themes.gohugo.io/ Edit hugo.toml, setting the \u0026ldquo;theme\u0026rdquo; property to the theme name. Create new content with the command \u0026ldquo;hugo new content \u0026lt;FILENAME\u0026gt;.\u0026rdquo;. Start the embedded web server with the command \u0026ldquo;hugo server \u0026ndash;buildDrafts\u0026rdquo;. See documentation at https://gohugo.io/.\ngit初始化仓库：\n1 2 cd myblog git init 撰写文章 1 hugo new content posts\\start.md content\\posts下会根据archetypes\\default.md中的模板新建一个名为start.md的文件。\n下载主题 在Hugo Themes中挑选合适主题，这里选择PaperMod主题：\n1 2 3 4 # 为git添加代理 git config --global http.proxy http://127.0.0.1:7890 # 克隆仓库 git submodule add https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod 注意把代理关掉，否则可能会无法连接（powershell里如果使用代理，git可能需要添加代理ip和端口号）。git submodule add \u0026lt;远程仓库URL\u0026gt; [本地路径]命令用于将另一个 Git 仓库作为​​子模块（Submodule）​​添加到当前项目中。子模块允许你在一个 Git 仓库中嵌套另一个独立的 Git 仓库，同时保持两者的提交历史隔离。\n将hugo.toml重命名为hugo.yml，这是Hugo的配置文件，.yml与.toml的语法略有不同：\n1 2 3 4 baseURL : \u0026#39;https://example.org/\u0026#39; #这里改成github pages的url languageCode : \u0026#39;zh-cn\u0026#39; title : \u0026#39;百川的个人博客\u0026#39; theme : \u0026#39;PaperMod\u0026#39; 本地预览：\n1 hugo server --buildDrafts ​​默认情况下​​，Hugo ​​不会渲染​​标记为 draft: true 的草稿内容。添加--buildDrafts参数（可简写为-D）后，Hugo 会​​强制包含所有草稿内容​​，方便你在写作时预览未完成的文章。此时 会返回链接http://localhost:1313/，复制到浏览器中打开即可查看博客。\n生成静态网站：\n1 hugo -D 在myblog文件夹中将产生一个public文件夹。\n将public文件夹上传到Github上，随后打开github pages产生的页面即可到达个人博客。\n参考资料 PaperMod官方文档\nPaperMod主题配置\nStack主题\n","date":"2025-05-23T23:00:00+08:00","permalink":"https://baichuan-blog.netlify.app/p/hugo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","title":"Hugo搭建个人博客"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","permalink":"https://baichuan-blog.netlify.app/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: 1 2 3 {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTeX globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTeX on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Inline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\nBlock math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$","date":"2019-03-08T00:00:00Z","permalink":"https://baichuan-blog.netlify.app/p/math-typesetting/","title":"Math Typesetting"},{"content":"考虑方程 $$\\begin{cases}\\partial_t \\rho +\\nabla\\cdot (\\rho \\mathbf{u}) = 0,\\\\\\rho\\partial_t\\mathbf{u}+\\rho\\mathbf{u}\\cdot\\nabla\\mathbf{u}=\\mathbf{g}.\\end{cases}\\tag{1}$$ 若时间离散采用$\\theta$-格式： $$\\begin{cases}\\frac{\\rho^{n+1}-\\rho^n}{\\Delta t}+\\nabla\\cdot(\\rho^{n+\\theta}\\mathbf{u}^{n+\\theta}) = 0,\\\\\\rho^{n+\\theta}\\frac{\\mathbf{u}^{n+1}-\\mathbf{u}^n}{\\Delta t}+\\rho^{n+\\theta}\\mathbf{u}^{n+\\theta}\\cdot\\nabla\\mathbf{u}^{n+\\theta}=\\mathbf{g}^{n+\\theta},\\end{cases}\\tag{2}$$ 其中 $$\\begin{cases}\\rho^{n+\\theta} =\\theta\\rho^{n+1}+(1-\\theta)\\rho^{n},\\\\\\mathbf{u}^{n+\\theta} =\\theta\\mathbf{u}^{n+1}+(1-\\theta)\\mathbf{u}^{n}.\\end{cases}\\tag{3}$$ 由于(2)是非线性系统，我们通过迭代法求解， 记$\\rho^{n+1,m},\\mathbf{u}^{n+1,m}$分别为由时间步$t^n$到$t^{n+1}$的第$m$次迭代解，$\\rho^{n+1,0}=\\rho^n,\\mathbf{u}^{n+1,0}=\\mathbf{u}^n$，将(2)线性化如下： $$\\begin{cases}\\frac{\\rho^{n+1,m+1}-\\rho^n}{\\Delta t}+\\nabla\\cdot(\\rho^{n+\\theta,m}\\mathbf{u}^{n+\\theta,m}) = 0,\\\\\\rho^{n+\\theta,m}\\frac{\\mathbf{u}^{n+1,m+1}-\\mathbf{u}^n}{\\Delta t}+\\rho^{n+\\theta,m}\\mathbf{u}^{n+\\theta,m}\\cdot\\nabla\\mathbf{u}^{n+\\theta,m}=\\mathbf{g}^{n+\\theta},\\end{cases}\\tag{4}$$ 其中 $$\\begin{cases}\\rho^{n+\\theta,m} =\\theta\\rho^{n+1,m}+(1-\\theta)\\rho^{n},\\\\\\mathbf{u}^{n+\\theta,m} =\\theta\\mathbf{u}^{n+1,m}+(1-\\theta)\\mathbf{u}^{n}.\\end{cases}\\tag{5}$$","date":"0001-01-01T00:00:00Z","permalink":"https://baichuan-blog.netlify.app/p/","title":""}]